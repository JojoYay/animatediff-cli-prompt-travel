{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262688db-ad61-4577-bd8c-69e0d21e4cc2",
   "metadata": {},
   "source": [
    "# AnimateDiff-V2V-GUI\n",
    "\n",
    "I have forked AnimateDiff prompt travel to add front GUI\n",
    "to do that, actually I have updated all those folder names etc\n",
    "\n",
    "What you need is slitely different.\n",
    "I'm keep syncing the repo so far and hopefully those basic function can also be available.\n",
    "(But result folder structure is a bit different)\n",
    "\n",
    "\n",
    "### How is it?\n",
    "\n",
    "[AnimateDiff with prompt travel](https://github.com/s9roll7/animatediff-cli-prompt-travel) + [ControlNet](https://github.com/lllyasviel/ControlNet) + [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)\n",
    "And now you have GUI\n",
    "\n",
    "### How to install\n",
    "\n",
    "Python 3.10 and git client must be installed.\n",
    "\n",
    "git clone the repo.\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/JojoYay/animatediff-cli-prompt-travel\n",
    "cd animatediff-cli-prompt-travel\n",
    "py -3.10 -m venv venv\n",
    "venv\\Scripts\\activate.bat\n",
    "set PYTHONUTF8=1\n",
    "python -m pip install --upgrade pip\n",
    "# Torch installation must be modified to suit the environment. (https://pytorch.org/get-started/locally/)\n",
    "python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "python -m pip install -e .\n",
    "\n",
    "```\n",
    "### Model, Motion Module, Lora prep\n",
    "You need to prepare Model, Motion Module, Lora and place them below  \n",
    "\n",
    "model : /animatediff-cli-prompt-travel/data/sd_models  \n",
    "LoRA : /animatediff-cli-prompt-travel/data/lora  \n",
    "motion module : /animatediff-cli-prompt-travel/data/motion_modules  \n",
    "\n",
    "when you start, you can see the URL in console.\n",
    "\n",
    "Click and enjoy making video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d80241a-61df-4dae-9f5b-f0a960437156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################google colab version##############################\n",
    "\n",
    "\n",
    "repo_storage_dir = '/contents/animatedif-v2v'         # Where to store your animatediff-cli-prompt-travel-related files.\n",
    "\n",
    "#+++GIT update++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_storage_dir = Path(repo_storage_dir_aj)\n",
    "\n",
    "%cd \"{Path(repo_storage_dir_aj, 'animatediff-cli-prompt-travel')}\"\n",
    "%cd /contents/animatedif-v2v/animatediff-cli-prompt-travel\n",
    "#!sudo apt-get update\n",
    "#!sudo apt-get install build-essential git yasm cmake libtool libx265-dev -y\n",
    "\n",
    "!pip install --upgrade pip\n",
    "#!pip uninstall onnxruntime onnxruntime-gpu onnxruntime_gpu -y\n",
    "#!pip install onnxruntime_gpu\n",
    "#!source venv/bin/activate\n",
    "!pip install -e .\n",
    "!python src/animatediff/front.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5de7eb-19ff-4045-bc68-c2d550370277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-02T01:11:44.171504Z",
     "iopub.status.busy": "2024-01-02T01:11:44.170995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/notebooks/storage': File exists\n",
      "ln: failed to create symbolic link '/notebooks/tmp': File exists\n",
      "Stored 'repo_dir' (str)\n",
      "Stored 'repo_storage_dir_aj' (str)\n",
      "/storage/aj/animatediff-cli-prompt-travel\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///storage/aj/animatediff-cli-prompt-travel\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate>=0.20.3 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (0.4.3)\n",
      "Collecting cmake>=3.25.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting diffusers==0.23.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading diffusers-0.23.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting einops>=0.6.1 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gdown>=4.6.6 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting ninja>=1.11.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (1.23.4)\n",
      "Collecting omegaconf>=2.3.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow<10.0.0,>=9.4.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich<14.0.0,>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (13.3.2)\n",
      "Collecting safetensors>=0.3.1 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m91.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: shellingham<2.0.0,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (1.5.0.post1)\n",
      "Collecting torch<2.2.0,>=2.1.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (0.12.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (0.13.1+cu116)\n",
      "Collecting transformers<4.35.0,>=4.30.2 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<1.0.0,>=0.9.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting controlnet-aux (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading controlnet_aux-0.0.7.tar.gz (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.4/202.4 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (3.6.1)\n",
      "Collecting ffmpeg-python>=0.2.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting mediapipe (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting xformers>=0.0.22.post7 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (1.26.15)\n",
      "Collecting yt-dlp (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading yt_dlp-2023.12.30-py2.py3-none-any.whl.metadata (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.7/160.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gradio (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio-4.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting onnxruntime-gpu (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading onnxruntime_gpu-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (1.5.0)\n",
      "Collecting segment-anything-hq==0.3 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading segment_anything_hq-0.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting groundingdino-py==0.4.0 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading groundingdino-py-0.4.0.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev361+gf0d537b.d20240102) (3.1.31)\n",
      "Collecting rembg[gpu] (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading rembg-2.0.53-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting moviepy (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pytorch-lightning (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting importlib-metadata (from diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading importlib_metadata-7.0.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102) (3.9.0)\n",
      "Collecting huggingface-hub>=0.13.2 (from diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading huggingface_hub-0.20.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102) (2.28.2)\n",
      "Collecting addict (from groundingdino-py==0.4.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting yapf (from groundingdino-py==0.4.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm (from groundingdino-py==0.4.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading timm-0.9.12-py3-none-any.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev361+gf0d537b.d20240102) (4.6.0.66)\n",
      "Collecting supervision==0.6.0 (from groundingdino-py==0.4.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading supervision-0.6.0-py3-none-any.whl (31 kB)\n",
      "Collecting pycocotools (from groundingdino-py==0.4.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev361+gf0d537b.d20240102) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev361+gf0d537b.d20240102) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev361+gf0d537b.d20240102) (5.4.1)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from ffmpeg-python>=0.2.0->animatediff==0.1.dev361+gf0d537b.d20240102) (0.18.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev361+gf0d537b.d20240102) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev361+gf0d537b.d20240102) (4.64.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev361+gf0d537b.d20240102) (4.11.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.3.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.0->animatediff==0.1.dev361+gf0d537b.d20240102) (4.5.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->animatediff==0.1.dev361+gf0d537b.d20240102) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->animatediff==0.1.dev361+gf0d537b.d20240102) (2.14.0)\n",
      "Collecting sympy (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102) (2023.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers<4.35.0,>=4.30.2->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->animatediff==0.1.dev361+gf0d537b.d20240102) (8.1.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from controlnet-aux->animatediff==0.1.dev361+gf0d537b.d20240102) (1.9.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from controlnet-aux->animatediff==0.1.dev361+gf0d537b.d20240102) (0.19.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->animatediff==0.1.dev361+gf0d537b.d20240102) (4.0.10)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fastapi (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading fastapi-0.108.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.8.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio_client-0.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting httpx (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102) (5.12.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gradio (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio-4.11.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.3 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio-4.10.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.9.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.2 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio_client-0.7.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio-4.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.1 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio_client-0.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio-4.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio_client-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: pip is still looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading gradio-4.4.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.1.2-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading gradio-4.1.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.1.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.0.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.0.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.6.1 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydub (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting semantic-version~=2.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading uvicorn-0.25.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev361+gf0d537b.d20240102) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev361+gf0d537b.d20240102) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev361+gf0d537b.d20240102) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev361+gf0d537b.d20240102) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev361+gf0d537b.d20240102) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev361+gf0d537b.d20240102) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->animatediff==0.1.dev361+gf0d537b.d20240102) (2022.7.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102) (1.4.0)\n",
      "Collecting attrs>=19.1.0 (from mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting flatbuffers>=2.0 (from mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting opencv-contrib-python (from mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading opencv_contrib_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102) (3.19.6)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting proglog<=1.0.0 (from moviepy->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->animatediff==0.1.dev361+gf0d537b.d20240102) (2.26.0)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting coloredlogs (from onnxruntime-gpu->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics>=0.7.0 (from pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting lightning-utilities>=0.8.0 (from pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading lightning_utilities-0.10.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102) (4.17.3)\n",
      "Collecting onnxruntime (from rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting opencv-python-headless (from rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting pooch (from rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading pooch-1.8.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pymatting (from rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading PyMatting-1.1.12-py3-none-any.whl.metadata (7.4 kB)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting mutagen (from yt-dlp->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pycryptodomex (from yt-dlp->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading pycryptodomex-3.19.1-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from yt-dlp->animatediff==0.1.dev361+gf0d537b.d20240102) (2019.11.28)\n",
      "Collecting requests (from diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3<2 (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of yt-dlp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting yt-dlp (from animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading yt_dlp-2023.11.16-py2.py3-none-any.whl.metadata (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting brotli (from yt-dlp->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->animatediff==0.1.dev361+gf0d537b.d20240102) (0.12.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102) (3.8.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->animatediff==0.1.dev361+gf0d537b.d20240102) (5.0.0)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.13.2 (from diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading huggingface_hub-0.20.0-py3-none-any.whl.metadata (12 kB)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.19.3-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.19.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.19.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.19.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy->animatediff==0.1.dev361+gf0d537b.d20240102) (67.6.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102) (0.19.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14.0.0,>=13.0.0->animatediff==0.1.dev361+gf0d537b.d20240102) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102) (2.8)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102) (1.15.1)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.6.6->animatediff==0.1.dev361+gf0d537b.d20240102) (2.4)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.33.0,>=0.29.0 (from fastapi->gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading starlette-0.32.0.post1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from pydantic<2.0.0,>=1.10.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->animatediff==0.1.dev361+gf0d537b.d20240102) (3.6.2)\n",
      "Collecting httpcore==1.* (from httpx->gradio->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->animatediff==0.1.dev361+gf0d537b.d20240102) (1.3.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata->diffusers==0.23.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102) (3.1.1)\n",
      "Collecting numba!=0.49.0 (from pymatting->rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "INFO: pip is looking at multiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->animatediff==0.1.dev361+gf0d537b.d20240102) (1.7.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux->animatediff==0.1.dev361+gf0d537b.d20240102) (2023.2.28)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux->animatediff==0.1.dev361+gf0d537b.d20240102) (1.4.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch<2.2.0,>=2.1.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting platformdirs>=2.5.0 (from pooch->rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading platformdirs-4.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tomli>=2.0.1 (from yapf->groundingdino-py==0.4.0->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning->animatediff==0.1.dev361+gf0d537b.d20240102) (1.3.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->animatediff==0.1.dev361+gf0d537b.d20240102) (2.21)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba!=0.49.0->pymatting->rembg[gpu]->animatediff==0.1.dev361+gf0d537b.d20240102)\n",
      "  Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Downloading diffusers-0.23.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading segment_anything_hq-0.3-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime_gpu-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yt_dlp-2023.11.16-py2.py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m954.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.25.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.108.0-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m764.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m127.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.19.1-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyMatting-1.1.12-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rembg-2.0.53-py3-none-any.whl (32 kB)\n",
      "Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: animatediff, groundingdino-py, antlr4-python3-runtime, controlnet-aux, moviepy, ffmpy\n",
      "  Building editable for animatediff (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for animatediff: filename=animatediff-0.1.dev361+gf0d537b.d20240102-0.editable-py3-none-any.whl size=6617 sha256=8f96c16f515c433d3e022ade93a6315e899c0988d584db6ae725038e178fbda2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-2i3hpfgj/wheels/a3/53/5e/a55600e8a54c8af9f77a7cefdd48769ca283a356ff22cc1550\n",
      "  Building wheel for groundingdino-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for groundingdino-py: filename=groundingdino_py-0.4.0-py2.py3-none-any.whl size=88739 sha256=b35e55bc3bdb7003ad2c83a06a6713231d570b626546eba5a7080c6f6a7975ac\n",
      "  Stored in directory: /root/.cache/pip/wheels/72/25/30/97b491abad279d329c62bef1e91bc56bf2fd40b22281068e1d\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=11b08ce0b5b5c2a0ed751394dafe337e5a2cce42a3aa7db95ec6767c50345d1e\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for controlnet-aux (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for controlnet-aux: filename=controlnet_aux-0.0.7-py3-none-any.whl size=274341 sha256=66b155abddad28730e02528e2d614235fa89a3ea3c09d637c03fdd72d920f66f\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/3e/93/6678b4c0bc2ec31d53409b25d4189cbb08bae843e8b2b78e52\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110728 sha256=ad389546135b2d7d0a0aed25ecb3f83d6b18dd4000b84118d1bda53a7f865831\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/32/2d/e10123bd88fbfc02fed53cc18c80a171d3c87479ed845fa7c1\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=a8aff7bd5610c2c132e395642313c6bc8d288abe2628486ed6a3b527b3ef645a\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
      "Successfully built animatediff groundingdino-py antlr4-python3-runtime controlnet-aux moviepy ffmpy\n",
      "Installing collected packages: sentencepiece, pydub, ninja, mpmath, flatbuffers, ffmpy, cmake, brotli, antlr4-python3-runtime, addict, zipp, websockets, urllib3, typing-extensions, triton, tomli, sympy, semantic-version, safetensors, python-multipart, pycryptodomex, proglog, platformdirs, pillow, orjson, opencv-python-headless, opencv-contrib-python, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mutagen, llvmlite, imageio_ffmpeg, humanfriendly, h11, ffmpeg-python, einops, decorator, attrs, aiofiles, uvicorn, typer, starlette, sounddevice, requests, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, lightning-utilities, importlib-metadata, httpcore, coloredlogs, yt-dlp, yapf, supervision, pymatting, pycocotools, pooch, onnxruntime-gpu, onnxruntime, nvidia-cusolver-cu12, moviepy, mediapipe, huggingface-hub, httpx, fastapi, altair, torch, tokenizers, rembg, gradio-client, gdown, diffusers, xformers, transformers, torchvision, torchmetrics, torchaudio, gradio, accelerate, timm, segment-anything-hq, pytorch-lightning, groundingdino-py, controlnet-aux, animatediff\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.97\n",
      "    Uninstalling sentencepiece-0.1.97:\n",
      "      Successfully uninstalled sentencepiece-0.1.97\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.1.1\n",
      "    Uninstalling platformdirs-3.1.1:\n",
      "      Successfully uninstalled platformdirs-3.1.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.4.2\n",
      "    Uninstalling typer-0.4.2:\n",
      "      Successfully uninstalled typer-0.4.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.2\n",
      "    Uninstalling pydantic-1.9.2:\n",
      "      Successfully uninstalled pydantic-1.9.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.13.1\n",
      "    Uninstalling huggingface-hub-0.13.1:\n",
      "      Successfully uninstalled huggingface-hub-0.13.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: gdown\n",
      "    Found existing installation: gdown 4.5.1\n",
      "    Uninstalling gdown-4.5.1:\n",
      "      Successfully uninstalled gdown-4.5.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1+cu116\n",
      "    Uninstalling torchvision-0.13.1+cu116:\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.12.1+cu116\n",
      "    Uninstalling torchaudio-0.12.1+cu116:\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.2.0 which is incompatible.\n",
      "spacy 3.4.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.10.13 which is incompatible.\n",
      "spacy 3.4.1 requires typer<0.5.0,>=0.3.0, but you have typer 0.9.0 which is incompatible.\n",
      "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.5.26 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.25.0 addict-2.4.0 aiofiles-23.2.1 altair-5.2.0 animatediff-0.1.dev361+gf0d537b.d20240102 antlr4-python3-runtime-4.9.3 attrs-23.2.0 brotli-1.1.0 cmake-3.28.1 coloredlogs-15.0.1 controlnet-aux-0.0.7 decorator-4.4.2 diffusers-0.23.0 einops-0.7.0 fastapi-0.108.0 ffmpeg-python-0.2.0 ffmpy-0.3.1 flatbuffers-23.5.26 gdown-4.7.1 gradio-3.50.2 gradio-client-0.6.1 groundingdino-py-0.4.0 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.17.3 humanfriendly-10.0 imageio_ffmpeg-0.4.9 importlib-metadata-7.0.1 lightning-utilities-0.10.0 llvmlite-0.41.1 mediapipe-0.10.9 moviepy-1.0.3 mpmath-1.3.0 mutagen-1.47.0 ninja-1.11.1.1 numba-0.58.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 onnxruntime-1.16.3 onnxruntime-gpu-1.16.3 opencv-contrib-python-4.9.0.80 opencv-python-headless-4.9.0.80 orjson-3.9.10 pillow-9.5.0 platformdirs-4.1.0 pooch-1.8.0 proglog-0.1.10 pycocotools-2.0.7 pycryptodomex-3.19.1 pydantic-1.10.13 pydub-0.25.1 pymatting-1.1.12 python-multipart-0.0.6 pytorch-lightning-2.1.3 rembg-2.0.53 requests-2.31.0 safetensors-0.4.1 segment-anything-hq-0.3 semantic-version-2.10.0 sentencepiece-0.1.99 sounddevice-0.4.6 starlette-0.32.0.post1 supervision-0.6.0 sympy-1.12 timm-0.9.12 tokenizers-0.14.1 tomli-2.0.1 torch-2.1.2 torchaudio-2.1.2 torchmetrics-1.2.1 torchvision-0.16.2 transformers-4.34.1 triton-2.1.0 typer-0.9.0 typing-extensions-4.9.0 urllib3-1.26.18 uvicorn-0.25.0 websockets-11.0.3 xformers-0.0.23.post1 yapf-0.40.2 yt-dlp-2023.11.16 zipp-3.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "diffuser_ver='0.23.0'\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://66b200c04c0f537460.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    }
   ],
   "source": [
    "repo_storage_dir_aj = '/storage/aj'         # Where to store your animatediff-cli-prompt-travel-related files.\n",
    "\n",
    "repo_dir = '/notebooks'    \n",
    "!ln -s /storage/ /notebooks/\n",
    "!ln -s /tmp/ /notebooks\n",
    "\n",
    "%store repo_dir \n",
    "# ===================================================================================================\n",
    "# Save variables to Jupiter's temp storage so we can access it even if the kernel restarts.\n",
    "%store repo_storage_dir_aj\n",
    "\n",
    "#+++GIT update++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_storage_dir_aj = Path(repo_storage_dir_aj)\n",
    "anime_diff_path = repo_storage_dir_aj / 'animatediff-cli-prompt-travel'\n",
    "\n",
    "%cd \"{Path(repo_storage_dir_aj, 'animatediff-cli-prompt-travel')}\"\n",
    "#!sudo apt-get update\n",
    "#!sudo apt-get install build-essential git yasm cmake libtool libx265-dev -y\n",
    "\n",
    "!pip install --upgrade pip\n",
    "#!pip uninstall onnxruntime onnxruntime-gpu onnxruntime_gpu -y\n",
    "#!pip install onnxruntime_gpu\n",
    "#!source venv/bin/activate\n",
    "!pip install -e .\n",
    "!python src/animatediff/front.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81183714-43b4-469f-901d-06fd7b186e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-09T11:12:04.979785Z",
     "iopub.status.busy": "2024-01-09T11:12:04.979006Z",
     "iopub.status.idle": "2024-01-09T11:12:04.988470Z",
     "shell.execute_reply": "2024-01-09T11:12:04.987883Z",
     "shell.execute_reply.started": "2024-01-09T11:12:04.979764Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Makima_real': 'lora/Makima_real.safetensors', '64x64v3-02': 'lora/64x64v3-02.safetensors', 'jolynejojo-000004': 'lora/jolynejojo-000004.safetensors', 'kid-v5': 'lora/kid-v5.safetensors', 'bsd_osamu-10': 'lora/bsd_osamu-10.safetensors', 'sd/_rL2170': 'lora/sd/_rL2170.safetensors', 'sd/Makima': 'lora/sd/Makima.safetensors', 'sd/Rem_Xmas': 'lora/sd/Rem_Xmas.safetensors', 'sd/MariV1': 'lora/sd/MariV1.safetensors', 'sd/mm3-000008': 'lora/sd/mm3-000008.safetensors', 'sd/opencoat': 'lora/sd/opencoat.safetensors', 'sd/covering_breasts_v0.1': 'lora/sd/covering_breasts_v0.1.safetensors', 'sd/koreanDollLikeness_v20': 'lora/sd/koreanDollLikeness_v20.safetensors', 'sd/japaneseDollLikeness_v15': 'lora/sd/japaneseDollLikeness_v15.safetensors', 'sd/LCM_LoRA_SD15': 'lora/sd/LCM_LoRA_SD15.safetensors', 'sd/Asuka': 'lora/sd/Asuka.safetensors', 'sd/arale': 'lora/sd/arale.safetensors', 'sd/sd-No.176-000014': 'lora/sd/sd-No.176-000014.safetensors', 'sd/shirtliftv1': 'lora/sd/shirtliftv1.safetensors', 'sd/SANTADRESS': 'lora/sd/SANTADRESS.safetensors', 'sd/HoshinoAi_v9': 'lora/sd/HoshinoAi_v9.safetensors', 'sd/erikaV1': 'lora/sd/erikaV1.safetensors', 'sd/pantypull-drop-r1': 'lora/sd/pantypull-drop-r1.safetensors', 'sd/Android_18': 'lora/sd/Android_18.safetensors', 'sd/top_view_perspective-10': 'lora/sd/top_view_perspective-10.safetensors', 'sd/lostedenTECHv3': 'lora/sd/lostedenTECHv3.safetensors', 'sd/side_view_perspective-10': 'lora/sd/side_view_perspective-10.safetensors', 'sd/roundassv1': 'lora/sd/roundassv1.safetensors', 'sd/add_detail': 'lora/sd/add_detail.safetensors', 'sd/innievag': 'lora/sd/innievag.safetensors', 'sd/Shanks-10': 'lora/sd/Shanks-10.safetensors', 'sd/AoiTodo001': 'lora/sd/AoiTodo001.safetensors', 'sd/Kobeni': 'lora/sd/Kobeni.safetensors', 'sd/CyberWorld_v1.1': 'lora/sd/CyberWorld_v1.1.safetensors', 'sd/aihoshinonova-10': 'lora/sd/aihoshinonova-10.safetensors', 'sd/raveclothesv1': 'lora/sd/raveclothesv1.safetensors', 'sd/nagahamaneru_JP_Actress_v1': 'lora/sd/nagahamaneru_JP_Actress_v1.safetensors', 'sd/MS_Real_POVPussyFromBelow_Lite': 'lora/sd/MS_Real_POVPussyFromBelow_Lite.safetensors', 'sd/V_bunnygirl_yw': 'lora/sd/V_bunnygirl_yw.safetensors', 'sd/yamato-10': 'lora/sd/yamato-10.safetensors', 'sd/lt_long_mm_16_64_frames': 'lora/sd/lt_long_mm_16_64_frames.ckpt', 'sd/ahegao_rolling_eyes-000014': 'lora/sd/ahegao_rolling_eyes-000014.safetensors', 'sd/bikinipull': 'lora/sd/bikinipull.safetensors', 'sd/yor': 'lora/sd/yor.safetensors', 'sd/sukuna': 'lora/sd/sukuna.safetensors', 'sd/exposepanty-strong': 'lora/sd/exposepanty-strong.safetensors', 'sd/LegUpPresenting': 'lora/sd/LegUpPresenting.safetensors', 'sd/newb_0.1': 'lora/sd/newb_0.1.safetensors', 'sd/OnesieFortnite_v1': 'lora/sd/OnesieFortnite_v1.safetensors', 'sd/AV_Suzumura': 'lora/sd/AV_Suzumura.safetensors', 'sd/back_view': 'lora/sd/back_view.safetensors', 'sd/Creampie_v11': 'lora/sd/Creampie_v11.safetensors', 'sd/irene_v70': 'lora/sd/irene_v70.safetensors', 'sd/nashiki': 'lora/sd/nashiki.safetensors', 'sd/hourglassv01': 'lora/sd/hourglassv01.safetensors', 'sd/suzuhonjo-000010': 'lora/sd/suzuhonjo-000010.safetensors', 'sd/ike2': 'lora/sd/ike2.safetensors', 'sd/bottom_view': 'lora/sd/bottom_view.safetensors', 'sd/nudify_xl': 'lora/sd/nudify_xl.safetensors', 'sd/ClockRealm': 'lora/sd/ClockRealm.safetensors', 'sd/sportsbrav1': 'lora/sd/sportsbrav1.safetensors', 'sd/KentoNanami001': 'lora/sd/KentoNanami001.safetensors', 'sd/iroro3': 'lora/sd/iroro3.safetensors', 'sd/chinaDollLikeness_v10': 'lora/sd/chinaDollLikeness_v10.safetensors', 'sd/fucsil1-000011': 'lora/sd/fucsil1-000011.safetensors', 'sd/mak2': 'lora/sd/mak2.safetensors'}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import pytz\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import yt_dlp\n",
    "import shutil\n",
    "import pytz\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "def find_safetensor_files(folder, suffix=''):\n",
    "    result_list = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".safetensors\") or file.endswith(\".ckpt\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                folder_name = os.path.relpath(root, folder)\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                \n",
    "                if folder_name != \".\":\n",
    "                    file_name = os.path.join(folder_name, file_name)\n",
    "                \n",
    "                result_name = f\"{suffix}{file_name}\"\n",
    "                result_path = os.path.relpath(file_path, folder)\n",
    "                if folder.startswith(\"data/\"):\n",
    "                    folder2 = folder[len(\"data/\"):]\n",
    "                # result_list.append((result_name, folder+'/'+result_path))\n",
    "                result_list[result_name] = folder2+'/'+result_path\n",
    "\n",
    "        for subdir in dirs:\n",
    "            subdir_path = os.path.join(root, subdir)\n",
    "            subdir_suffix = f\"{suffix}{subdir}/\" if suffix else f\"{subdir}/\"\n",
    "            # result_list.append(find_safetensor_files(subdir_path, subdir_suffix))\n",
    "            result_list = result_list | find_safetensor_files(subdir_path, subdir_suffix)\n",
    "    # result_list.sort(key=lambda x: x[0])  # file_name でソート\n",
    "    return result_list\n",
    "\n",
    "print(find_safetensor_files('data/lora'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c658ed-febb-4b59-a8ba-ee259d0f4335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-10T01:10:31.406705Z",
     "iopub.status.busy": "2024-01-10T01:10:31.406026Z",
     "iopub.status.idle": "2024-01-10T01:10:42.613895Z",
     "shell.execute_reply": "2024-01-10T01:10:42.613348Z",
     "shell.execute_reply.started": "2024-01-10T01:10:31.406681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://a783e350f47a626044.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a783e350f47a626044.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "value_map = {\"cat\":\"cat_value\",\n",
    "             \"dog\":\"dog_value\",\n",
    "             \"bird\":\"bird_value\"}\n",
    "\n",
    "def sentence_builder(value):\n",
    "    return value_map[value]\n",
    "\n",
    "demo = gr.Interface(\n",
    "    sentence_builder,\n",
    "    [\n",
    "        gr.Dropdown(\n",
    "            value_map.keys(), value=list(value_map.keys())[0] if list(value_map.keys()) != [] else None, label=\"Animal\"\n",
    "        )\n",
    "    ],\n",
    "    \"text\"\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ad305e-9354-4a72-90b8-801f5732c032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-18T05:25:49.770846Z",
     "iopub.status.busy": "2023-12-18T05:25:49.770334Z",
     "iopub.status.idle": "2023-12-18T05:26:09.561648Z",
     "shell.execute_reply": "2023-12-18T05:26:09.560546Z",
     "shell.execute_reply.started": "2023-12-18T05:25:49.770825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/aj/animatediff-cli-prompt-travel\n",
      "[TikTok] Extracting URL: https://www.tiktok.com/@fukada0318/video/7290017445211819266\n",
      "[TikTok] 7290017445211819266: Downloading video feed\n",
      "[info] 7290017445211819266: Downloading 1 format(s): bytevc1_1080p_2508203-2\n",
      "[download] ./data/video/dance00023.mp4 has already been downloaded\n",
      "[download] 100% of    6.75MiB\n",
      "impl\n",
      "video1: ./data/video/dance00023.mp4\n",
      "config already exists. skip create-config\n",
      "Using generation config: stylize/dance00023/fg_00_dance00023/prompt.json\n",
      "is_sdxl=False\n",
      "is_v2=True\n",
      "Using base model: runwayml/stable-diffusion-v1-5\n",
      "Will save outputs to ./stylize/dance00023/fg_00_dance00023/2023-12-18_13-25-88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing images (animatediff_controlnet): 100%|██████████| 140/140 [00:00<00:00, 962.00it/s]\n",
      "Preprocessing images (controlnet_openpose):   0%|          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing images (controlnet_openpose): 100%|██████████| 140/140 [00:15<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline already loaded, skipping initialization\n",
      "loading c='animatediff_controlnet' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for conv_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for conv_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.5.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.5.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.downsamplers.0.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.downsamplers.0.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv_shortcut.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv_shortcut.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.downsamplers.0.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.downsamplers.0.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv_shortcut.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv_shortcut.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.downsamplers.0.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.downsamplers.0.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.5.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.5.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.6.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.6.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.7.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.7.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.8.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.8.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.9.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.9.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.10.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.10.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.11.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.11.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_mid_block.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_mid_block.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "loading c='controlnet_openpose' model\n",
      "Pipeline already on the correct device, skipping device transfer\n",
      "c='animatediff_controlnet' / []\n",
      "c='controlnet_openpose' / []\n",
      "Saving prompt config to output directory\n",
      "Initialization complete!\n",
      "Generating 1 animations\n",
      "Running generation 1 of 1\n",
      "Generation seed: 74699447653086840\n",
      "len( region_condi_list )=1\n",
      "len( region_list )=1\n",
      "apply_lcm_lora=True\n",
      "controlnet_for_region=True\n",
      "multi_uncond_mode=True\n",
      "unet_batch_size=1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"LayerNormKernelImpl\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m anime_diff_path \u001b[38;5;241m=\u001b[39m repo_storage_dir_aj \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimatediff-cli-prompt-travel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     56\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mPath(repo_storage_dir_aj, \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124manimatediff-cli-prompt-travel\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43murls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_if_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelete_if_exists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_refine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_refine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     62\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/execute.py:49\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(videos, configs, urls, delete_if_exists, is_test, is_refine, bg_config)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/notebooks\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     48\u001b[0m     config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/notebooks\u001b[39m\u001b[38;5;124m\"\u001b[39m):]\n\u001b[0;32m---> 49\u001b[0m \u001b[43mexecute_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaved_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_if_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelete_if_exists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_refine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_refine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/execute.py:98\u001b[0m, in \u001b[0;36mexecute_impl\u001b[0;34m(video, config, delete_if_exists, is_test, is_refine, bg_config)\u001b[0m\n\u001b[1;32m     95\u001b[0m save_config_path\u001b[38;5;241m.\u001b[39mwrite_text(model_config\u001b[38;5;241m.\u001b[39mjson(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_test:\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstylize_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstylize_fg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bg_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         generate(stylize_dir\u001b[38;5;241m=\u001b[39mstylize_bg_dir, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py:615\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(stylize_dir, length, frame_offset)\u001b[0m\n\u001b[1;32m    611\u001b[0m         tmp_config_path\u001b[38;5;241m.\u001b[39mwrite_text(model_config\u001b[38;5;241m.\u001b[39mjson(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    612\u001b[0m         config_org \u001b[38;5;241m=\u001b[39m tmp_config_path\n\u001b[0;32m--> 615\u001b[0m     output_0_dir \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_org\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlength\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverlap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstride\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstylize_dir\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m#    try:\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m#        # フォルダが存在する場合のみ削除\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m#        shutil.rmtree(output_0_dir.parent / f\"{time_str}_{0:02d}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m#    except Exception as e:\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m#        print(f\"Error occurred while deleting Output folder: {e}\")\u001b[39;00m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py:445\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(config_path, width, height, length, context, overlap, stride, repeats, device, use_xformers, force_half_vae, out_dir, no_frames, save_merged, version)\u001b[0m\n\u001b[1;32m    440\u001b[0m seed \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mseed[idx \u001b[38;5;241m%\u001b[39m num_seeds]\n\u001b[1;32m    442\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 445\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg2img_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg2img_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_condi_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_condi_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_sdxl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_sdxl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradual_latent_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradual_latent_hires_fix_map\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    479\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py:1541\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(pipeline, n_prompt, seed, steps, guidance_scale, unet_batch_size, width, height, duration, idx, out_dir, context_frames, context_stride, context_overlap, context_schedule, clip_skip, controlnet_map, controlnet_image_map, controlnet_type_map, controlnet_ref_map, controlnet_no_shrink, no_frames, img2img_map, ip_adapter_config_map, region_list, region_condi_list, output_map, is_single_prompt_mode, is_sdxl, apply_lcm_lora, gradual_latent_map)\u001b[0m\n\u001b[1;32m   1538\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;250m \u001b[39mregion_condi_list\u001b[38;5;250m \u001b[39m)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1539\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;250m \u001b[39mregion_list\u001b[38;5;250m \u001b[39m)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1541\u001b[0m pipeline_output \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_max_samples_on_vram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_samples_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_samples_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_max_models_on_vram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_models_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_models_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_is_loop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_loop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_loop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg2img_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg2img_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_condi_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_condi_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradual_latent_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradual_latent_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreview_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration complete, saving...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1575\u001b[0m save_fn(pipeline_output, out_file\u001b[38;5;241m=\u001b[39mout_file)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py:2481\u001b[0m, in \u001b[0;36mAnimationPipeline.__call__\u001b[0;34m(self, height, width, num_inference_steps, guidance_scale, unet_batch_size, negative_prompt, video_length, num_videos_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, context_frames, context_stride, context_overlap, context_schedule, clip_skip, controlnet_type_map, controlnet_image_map, controlnet_ref_map, controlnet_no_shrink, controlnet_max_samples_on_vram, controlnet_max_models_on_vram, controlnet_is_loop, img2img_map, ip_adapter_config_map, region_list, region_condi_list, interpolation_factor, is_single_prompt_mode, apply_lcm_lora, gradual_latent_map, **kwargs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;66;03m# 3. Encode input prompt\u001b[39;00m\n\u001b[1;32m   2476\u001b[0m text_encoder_lora_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2477\u001b[0m     cross_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2478\u001b[0m )\n\u001b[0;32m-> 2481\u001b[0m prompt_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPromptEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#latents_device,\u001b[39;49;00m\n\u001b[1;32m   2485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_condi_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_uncond_mode\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mip_adapter:\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mip_adapter\u001b[38;5;241m.\u001b[39mdelete_encoder()\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py:101\u001b[0m, in \u001b[0;36mPromptEncoder.__init__\u001b[0;34m(self, pipe, device, latents_device, num_videos_per_prompt, do_classifier_free_guidance, region_condi_list, negative_prompt, is_signle_prompt_mode, clip_skip, multi_uncond_mode)\u001b[0m\n\u001b[1;32m     98\u001b[0m     prompt_nums\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;28mlen\u001b[39m(_prompt_list) )\n\u001b[1;32m     99\u001b[0m     prompt_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _prompt_list\n\u001b[0;32m--> 101\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device \u001b[38;5;241m=\u001b[39m latents_device)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_embeds_dtype \u001b[38;5;241m=\u001b[39m prompt_embeds\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py:821\u001b[0m, in \u001b[0;36mAnimationPipeline._encode_prompt\u001b[0;34m(self, prompt, device, num_videos_per_prompt, do_classifier_free_guidance, negative_prompt, max_embeddings_multiples, prompt_embeds, negative_prompt_embeds, clip_skip)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance \u001b[38;5;129;01mand\u001b[39;00m negative_prompt_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m         negative_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_convert_prompt(negative_prompt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[0;32m--> 821\u001b[0m prompt_embeds1, negative_prompt_embeds1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_weighted_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43muncond_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_embeddings_multiples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_embeddings_multiples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m prompt_embeds1\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/lpw_stable_diffusion.py:343\u001b[0m, in \u001b[0;36mget_weighted_text_embeddings\u001b[0;34m(pipe, prompt, uncond_prompt, max_embeddings_multiples, no_boseos_middle, skip_parsing, skip_weighting, clip_skip)\u001b[0m\n\u001b[1;32m    340\u001b[0m     uncond_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(uncond_tokens, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mpipe\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# get the embeddings\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_unweighted_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_boseos_middle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_boseos_middle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m prompt_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(prompt_weights, dtype\u001b[38;5;241m=\u001b[39mtext_embeddings\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtext_embeddings\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m uncond_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/lpw_stable_diffusion.py:237\u001b[0m, in \u001b[0;36mget_unweighted_text_embeddings\u001b[0;34m(pipe, text_input, chunk_length, no_boseos_middle, clip_skip)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pipe\u001b[38;5;241m.\u001b[39mtext_encoder, CLIPSkipTextModel):\n\u001b[0;32m--> 237\u001b[0m         text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m         text_embeddings \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mtext_encoder(text_input)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/clip.py:153\u001b[0m, in \u001b[0;36mCLIPSkipTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict, clip_skip)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/clip.py:69\u001b[0m, in \u001b[0;36mCLIPSkipTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict, clip_skip)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m _expand_mask(attention_mask, hidden_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 69\u001b[0m encoder_outputs: BaseModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# take the hidden state from the Nth-to-last layer of the encoder, where N = clip_skip\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# clip_skip=1 means take the hidden state from the last layer as with CLIPTextTransformer\u001b[39;00m\n\u001b[1;32m     80\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39mclip_skip]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py:656\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    649\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    650\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    651\u001b[0m         hidden_states,\n\u001b[1;32m    652\u001b[0m         attention_mask,\n\u001b[1;32m    653\u001b[0m         causal_attention_mask,\n\u001b[1;32m    654\u001b[0m     )\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 656\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py:385\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 385\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    387\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    388\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    389\u001b[0m     causal_attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[1;32m    390\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    392\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"LayerNormKernelImpl\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "############################################################\n",
    "#ここにビデオのURLを入力#######################################\n",
    "urls = [\n",
    "#    \"https://www.tiktok.com/@assen047/video/7273517043876433158\",   #dance00010\n",
    "#    \"https://www.tiktok.com/@xiavigor__/video/7292363704266427653\", #dance00011\n",
    "#    \"https://www.tiktok.com/@admin_break/video/7306911339673324806\", #dance00012\n",
    "#    \"https://www.tiktok.com/@admin_break/video/7309132061330181381\",#dance00013\n",
    "#    \"https://www.tiktok.com/@xntonio9/video/7266524869783784737\",   #dance00014 soccer\n",
    "#    \"https://www.tiktok.com/@chiranjit_babu/video/7309784153761795329\", #dance00015 indo dance ３人でるから没\n",
    " #   \"https://twitter.com/TaichiZhe/status/1732774857807646772\", #dance00016 kick\n",
    " #   \"https://twitter.com/tongbeijapan/status/1731906333081841755\",#dance00017 punch2\n",
    " #   \"https://www.tiktok.com/@chiranjit_babu/video/7287774575062945042\",#dance00018 indo dance2\n",
    " #   \"https://www.tiktok.com/@chiranjit_babu/video/7210939347212700936\", #dance00019 indo dance3    \n",
    "#    \"https://www.tiktok.com/@fantasista283/video/7309087809829162247\", #dance00020 soccer    \n",
    "#    \"https://www.tiktok.com/@lia.lewis/video/7297191511010118945\", #dance00021 soccer girl\n",
    "#    \"https://www.tiktok.com/@raynavallandingham/video/7309946333593439530\", #dance00022 girl kunfoo\n",
    "    \"https://www.tiktok.com/@fukada0318/video/7290017445211819266\", #dance00023 eimi fukada\n",
    "#    \"https://www.tiktok.com/@ai_hinahina/video/7312648648553204999\", #dance00027\n",
    "]\n",
    "videos = [\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004',\n",
    "#    '/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/video/dance31.mp4'\n",
    "#    '/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/video/dance32.mp4',\n",
    "#    '/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/video/dance33.mp4',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00001.mp4',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00002.mp4',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00003.mp4',\n",
    "#     '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00004.mp4',\n",
    "]\n",
    "configs = [\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/kobeni.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/real_base.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/just_anime.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/sukuna.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/todo.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/nanamin.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/jotaro.json',\n",
    "    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/real_base2.json',\n",
    "]\n",
    "#bg_config = '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/bg/BeachLCM.json'\n",
    "bg_config = None\n",
    "\n",
    "delete_if_exists = False\n",
    "is_test = True\n",
    "is_refine = False\n",
    "############################################################\n",
    "from animatediff.execute import execute\n",
    "from pathlib import Path\n",
    "\n",
    "repo_storage_dir_aj = Path('/storage/aj')\n",
    "anime_diff_path = repo_storage_dir_aj / 'animatediff-cli-prompt-travel'\n",
    "\n",
    "%cd \"{Path(repo_storage_dir_aj, 'animatediff-cli-prompt-travel')}\"\n",
    "\n",
    "execute(videos=videos, configs=configs,urls=urls, delete_if_exists=delete_if_exists, is_test=is_test, is_refine=is_refine, bg_config=bg_config)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"実行時間: {execution_time}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47adb47e-e655-419a-a3bb-55adfaaa396a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-01T17:14:19.443138Z",
     "iopub.status.busy": "2024-01-01T17:14:19.442270Z",
     "iopub.status.idle": "2024-01-01T17:18:33.330271Z",
     "shell.execute_reply": "2024-01-01T17:18:33.329413Z",
     "shell.execute_reply.started": "2024-01-01T17:14:19.443111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "diffuser_ver='0.23.0'\n",
      "Using generation config: config/from_ui/20240102_0113.json\n",
      "is_sdxl=False\n",
      "is_v2=True\n",
      "Using base model: runwayml/stable-diffusion-v1-5\n",
      "Will save outputs to ./output/2024-01-02_01-14-88\n",
      "Checking motion module...\n",
      "Loading tokenizer...\n",
      "Loading text encoder...\n",
      "Loading VAE...\n",
      "Loading UNet...\n",
      "Loaded 453.20928M-parameter motion module\n",
      "Using scheduler \"euler_a\" (EulerAncestralDiscreteScheduler)\n",
      "Loading weights from /storage/aj/animatediff-cli-prompt-travel/data/sd_models/sd/majicmixRealistic_betterV2V25.safetensors\n",
      "Merging weights into UNet...\n",
      "Loading vae from /storage/aj/animatediff-cli-prompt-travel/data/vae/sd/vae-ft-mse-840000-ema-pruned.safetensors\n",
      "Creating AnimationPipeline...\n",
      "lora_path=PosixPath('/storage/aj/animatediff-cli-prompt-travel/data/models/lcm_lora/sd15/pytorch_lora_weights.safetensors')\n",
      "create LoRA network from weights\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc2 (not found in modules_dim)\n",
      "create LoRA for Text Encoder: 0 modules.\n",
      "skipped 72 modules because of missing weight for text encoder.\n",
      "create LoRA for U-Net: 278 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "No TI embeddings found\n",
      "Sending pipeline to device \"cuda\"\n",
      "Selected data types: unet_dtype=torch.float16, tenc_dtype=torch.float16, vae_dtype=torch.bfloat16\n",
      "Using channels_last memory format for UNet and VAE\n",
      "alisalisalisalisalisalisalisalisalis /storage/aj/animatediff-cli-prompt-travel/data/../stylize/aaaw/00_ipadapter\n",
      "Preprocessing images (ip_adapter): 100%|██████████| 1/1 [00:00<00:00, 58.19it/s]\n",
      "Saving prompt config to output directory\n",
      "Initialization complete!\n",
      "Generating 1 animations\n",
      "Running generation 1 of 1\n",
      "Generation seed: 7\n",
      "len( region_condi_list )=1\n",
      "len( region_list )=1\n",
      "apply_lcm_lora=True\n",
      "controlnet_for_region=True\n",
      "multi_uncond_mode=True\n",
      "unet_batch_size=1\n",
      "prompt_encoder.get_condi_size()=2\n",
      "100%|█████████████████████████████████████████| 187/187 [03:00<00:00,  1.04it/s]\n",
      "Generation complete, saving...\n",
      "Creating ffmpeg encoder...\n",
      "Encoding interpolated frames with ffmpeg...\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from '/storage/aj/animatediff-cli-prompt-travel/output/2024-01-02_01-14-88/00-2024-01-02_01-14/%08d.png':\n",
      "  Duration: 00:00:08.00, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 512x768, 16 fps, 16 tbr, 16 tbn, 16 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 (png) -> fps\n",
      "  fps -> Stream #0:0 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=6 deblock=1:1:1 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=0.40:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=5 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=16 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=10.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:0.60\n",
      "Output #0, mp4, to '/storage/aj/animatediff-cli-prompt-travel/output/2024-01-02_01-14-88/00_2024-01-02_01-14_.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 512x768, q=-1--1, 16 fps, 16384 tbn, 16 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  128 fps= 90 q=-1.0 Lsize=    3667kB time=00:00:07.81 bitrate=3844.6kbits/s speed=5.47x    \n",
      "video:3664kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.060444%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mframe I:3     Avg QP: 8.95  size: 55859\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mframe P:40    Avg QP:10.65  size: 39499\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mframe B:85    Avg QP:11.58  size: 23577\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mconsecutive B-frames:  6.2% 15.6% 21.1% 28.1% 19.5%  9.4%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mmb I  I16..4: 15.0% 62.7% 22.3%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mmb P  I16..4: 10.6% 39.0% 11.4%  P16..4: 11.5% 15.0% 10.6%  0.0%  0.0%    skip: 1.8%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mmb B  I16..4:  2.3%  9.5%  3.9%  B16..8: 25.5% 22.8%  9.3%  direct:18.5%  skip: 8.1%  L0:48.6% L1:34.2% BI:17.2%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0m8x8 transform intra:62.7% inter:68.9%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mcoded y,uvDC,uvAC intra: 71.4% 96.1% 95.1% inter: 52.4% 82.6% 55.1%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mi16 v,h,dc,p: 66% 13%  5% 16%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 22% 18% 13%  6%  7%  9%  7% 10%  7%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 24% 17% 12%  6% 11% 11%  7%  6%  4%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mi8c dc,h,v,p: 42% 20% 23% 15%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mWeighted P-Frames: Y:15.0% UV:12.5%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mref P L0: 41.2%  8.0% 22.9% 13.1%  8.1%  5.5%  1.1%  0.1%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mref B L0: 70.8% 15.3%  9.4%  3.5%  1.1%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mref B L1: 95.5%  4.5%\n",
      "\u001b[1;36m[libx264 @ 0x559d985149c0] \u001b[0mkb/s:3751.57\n",
      "Saved sample to output/2024-01-02_01-14-88/00_2024-01-02_01-14_\n",
      "Generation complete!\n",
      "Done, exiting...\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!animatediff refine /storage/comfyUI/ComfyUI/output/2023-12-11/ADF/094059 -c /storage/aj/animatediff-cli-prompt-travel/stylize/jojo-dance00017/fg_00_jojo/prompt.json -W 768\n",
    "#!animatediff stylize composite /storage/aj/animatediff-cli-prompt-travel/stylize/real_base-dance00022 --simple_composite\n",
    "!animatediff generate -c /storage/aj/animatediff-cli-prompt-travel/config/from_ui/20240102_0113.json -W 512 -H 768 -L 128 -C 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e313c7d5-a31f-4959-bf8b-a3ff3b96c990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T10:04:37.169062Z",
     "iopub.status.busy": "2023-12-14T10:04:37.168571Z",
     "iopub.status.idle": "2023-12-14T10:06:40.155047Z",
     "shell.execute_reply": "2023-12-14T10:06:40.154443Z",
     "shell.execute_reply.started": "2023-12-14T10:04:37.169038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2;36m10:04:44\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mdiffuser_ver\u001b[0m=\u001b[32m'0.23.0'\u001b[0m                               \u001b]8;id=116641;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=364288;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#101\u001b\\\u001b[2m101\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using generation config:                            \u001b]8;id=847610;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=912925;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#314\u001b\\\u001b[2m314\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         stylize/real_base2-dance00025/fg_00_real_base2/prom \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         pt.json                                             \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mis_sdxl\u001b[0m=\u001b[3;91mFalse\u001b[0m                                      \u001b]8;id=558900;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py\u001b\\\u001b[2mutil.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=325426;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py#600\u001b\\\u001b[2m600\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mis_v2\u001b[0m=\u001b[3;92mTrue\u001b[0m                                         \u001b]8;id=611568;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py\u001b\\\u001b[2mutil.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=196034;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py#578\u001b\\\u001b[2m578\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using base model: runwayml/stable-diffusion-v1-\u001b[1;36m5\u001b[0m    \u001b]8;id=397933;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=257313;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#341\u001b\\\u001b[2m341\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Will save outputs to                                \u001b]8;id=352884;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=899894;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#351\u001b\\\u001b[2m351\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         .\u001b[35m/stylize/real_base2-dance00025/fg_00_real_base2/\u001b[0m\u001b[95m20\u001b[0m \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[95m23-12-14_18-04-88\u001b[0m                                   \u001b[2m          \u001b[0m\n",
      "\u001b[2KPreprocessing images (animatediff_controlnet)\u001b[35m   0%\u001b[0m \u001b[90m━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?  \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (animatediff_controlnet)\u001b[35m   0%\u001b[0m \u001b[90m━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?  \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (animatediff_controlnet)\u001b[35m   5%\u001b[0m \u001b[90m━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?  \u001b[0m ]\n",
      "                                                                           \u001b[31mit…\u001b[0m  \n",
      "\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_c\n",
      "ollection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not \n",
      "in available provider names.Available providers: 'AzureExecutionProvider, \n",
      "CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "Preprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   6%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m16/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "                                                                          \u001b[31mit/s\u001b[0m  \n",
      "\u001b[?25h\u001b[2;36m10:05:03\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Checking motion module\u001b[33m...\u001b[0m                      \u001b]8;id=51432;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=646210;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#612\u001b\\\u001b[2m612\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading tokenizer\u001b[33m...\u001b[0m                           \u001b]8;id=702710;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=486314;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#636\u001b\\\u001b[2m636\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading text encoder\u001b[33m...\u001b[0m                        \u001b]8;id=715073;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=286591;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#638\u001b\\\u001b[2m638\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:05\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading VAE\u001b[33m...\u001b[0m                                 \u001b]8;id=928182;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=766367;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#640\u001b\\\u001b[2m640\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading UNet\u001b[33m...\u001b[0m                                \u001b]8;id=207389;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=251631;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#642\u001b\\\u001b[2m642\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:19\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[1;36m453.\u001b[0m20928M-parameter motion module          \u001b]8;id=884207;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py\u001b\\\u001b[2munet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=230752;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py#578\u001b\\\u001b[2m578\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:20\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m gradual_latent_hires_fix enable                \u001b]8;id=37095;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=478588;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#656\u001b\\\u001b[2m656\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m model_config.\u001b[33mscheduler\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDiffusionScheduler.k_d\u001b[0m \u001b]8;id=333169;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=60529;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#657\u001b\\\u001b[2m657\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[1;95mpmpp_2m:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'k_dpmpp_2m'\u001b[0m\u001b[1m>\u001b[0m                         \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m If you are forced to exit with an error,       \u001b]8;id=403274;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=554568;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#658\u001b\\\u001b[2m658\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         change to euler_a or lcm                       \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using scheduler \u001b[32m\"k_dpmpp_2m\"\u001b[0m                   \u001b]8;id=221630;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=284086;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#662\u001b\\\u001b[2m662\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[1m(\u001b[0mDPMSolverMultistepScheduler\u001b[1m)\u001b[0m                  \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading weights from                           \u001b]8;id=964379;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=519992;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#667\u001b\\\u001b[2m667\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/storage/aj/animatediff-cli-prompt-travel/data\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/../../../stable-diffusion/stable-diffusion-we\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35mbui/models/Stable-diffusion/\u001b[0m\u001b[95mmajicmixRealistic_\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[95mbetterV2V25.safetensors\u001b[0m                        \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m10:05:23\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Merging weights into UNet\u001b[33m...\u001b[0m                   \u001b]8;id=34568;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=431219;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#684\u001b\\\u001b[2m684\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:25\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Creating AnimationPipeline\u001b[33m...\u001b[0m                  \u001b]8;id=19009;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=689702;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#734\u001b\\\u001b[2m734\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mlora_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'/storage/aj/animatediff-cli-pr\u001b[0m \u001b]8;id=321329;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=324894;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[32mompt-travel/data/models/lcm_lora/sd15/pytorch_lora_\u001b[0m \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[32mweights.safetensors'\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m          \u001b[0m\n",
      "create LoRA network from weights\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc2 (not found in modules_dim)\n",
      "create LoRA for Text Encoder: 0 modules.\n",
      "skipped 72 modules because of missing weight for text encoder.\n",
      "create LoRA for U-Net: 278 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "\u001b[2;36m10:05:44\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m No TI embeddings found                               \u001b]8;id=404883;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py\u001b\\\u001b[2mti.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=11571;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py#104\u001b\\\u001b[2m104\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading \u001b[33mc\u001b[0m=\u001b[32m'animatediff_controlnet'\u001b[0m model       \u001b]8;id=519990;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=177548;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\u001b\\\u001b[2m783\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:48\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading \u001b[33mc\u001b[0m=\u001b[32m'controlnet_openpose'\u001b[0m model          \u001b]8;id=685411;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=829860;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\u001b\\\u001b[2m783\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:49\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sending pipeline to device \u001b[32m\"cuda\"\u001b[0m               \u001b]8;id=319268;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/pipeline.py\u001b\\\u001b[2mpipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=104318;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/pipeline.py#33\u001b\\\u001b[2m33\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Selected data types: \u001b[33munet_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16,    \u001b]8;id=404810;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py\u001b\\\u001b[2mdevice.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=53984;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[33mtenc_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16,                         \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[33mvae_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.bfloat16                          \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using channels_last memory format for UNet and   \u001b]8;id=143752;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py\u001b\\\u001b[2mdevice.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=934594;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py#111\u001b\\\u001b[2m111\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         VAE                                              \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m10:05:54\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mc\u001b[0m=\u001b[32m'animatediff_controlnet'\u001b[0m \u001b[35m/\u001b[0m \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                     \u001b]8;id=75909;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=196786;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#413\u001b\\\u001b[2m413\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mc\u001b[0m=\u001b[32m'controlnet_openpose'\u001b[0m \u001b[35m/\u001b[0m \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                        \u001b]8;id=692440;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=565138;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#413\u001b\\\u001b[2m413\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Saving prompt config to output directory            \u001b]8;id=724563;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=667883;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#416\u001b\\\u001b[2m416\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Initialization complete!                            \u001b]8;id=263065;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=386092;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#424\u001b\\\u001b[2m424\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generating \u001b[1;36m1\u001b[0m animations                             \u001b]8;id=956300;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=234721;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#425\u001b\\\u001b[2m425\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Running generation \u001b[1;36m1\u001b[0m of \u001b[1;36m1\u001b[0m                           \u001b]8;id=95747;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=556488;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#435\u001b\\\u001b[2m435\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generation seed: \u001b[1;36m74699447653086840\u001b[0m                  \u001b]8;id=218190;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=781208;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#441\u001b\\\u001b[2m441\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;35mlen\u001b[0m\u001b[1m(\u001b[0m region_condi_list \u001b[1m)\u001b[0m=\u001b[1;36m1\u001b[0m                    \u001b]8;id=53158;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=848234;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1532\u001b\\\u001b[2m1532\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;35mlen\u001b[0m\u001b[1m(\u001b[0m region_list \u001b[1m)\u001b[0m=\u001b[1;36m1\u001b[0m                          \u001b]8;id=147419;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=259069;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1533\u001b\\\u001b[2m1533\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mapply_lcm_lora\u001b[0m=\u001b[3;92mTrue\u001b[0m                          \u001b]8;id=288705;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=344756;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2407\u001b\\\u001b[2m2407\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mcontrolnet_for_region\u001b[0m=\u001b[3;92mTrue\u001b[0m                   \u001b]8;id=486031;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=86655;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2435\u001b\\\u001b[2m2435\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mmulti_uncond_mode\u001b[0m=\u001b[3;92mTrue\u001b[0m                       \u001b]8;id=110847;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=851892;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2436\u001b\\\u001b[2m2436\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33munet_batch_size\u001b[0m=\u001b[1;36m1\u001b[0m                            \u001b]8;id=558369;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=121403;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2437\u001b\\\u001b[2m2437\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:55\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;35mprompt_encoder.get_condi_size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m=\u001b[1;36m2\u001b[0m            \u001b]8;id=895987;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=221921;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2500\u001b\\\u001b[2m2500\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m0 it/s\u001b[0m ] \u001b[31m0 it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[2;36m10:06:34\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generation complete, saving\u001b[33m...\u001b[0m                \u001b]8;id=467011;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=91221;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1566\u001b\\\u001b[2m1566\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:06:36\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Creating ffmpeg encoder\u001b[33m...\u001b[0m                    \u001b]8;id=275420;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=738596;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1442\u001b\\\u001b[2m1442\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Encoding interpolated frames with ffmpeg\u001b[33m...\u001b[0m   \u001b]8;id=496216;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=937916;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1466\u001b\\\u001b[2m1466\u001b[0m\u001b]8;;\u001b\\\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from '/storage/aj/animatediff-cli-prompt-travel/stylize/real_base2-dance00025/fg_00_real_base2/2023-12-14_18-04-88/00-2023-12-14_18-05/%08d.png':\n",
      "  Duration: 00:00:01.07, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 512x896, 15 fps, 15 tbr, 15 tbn, 15 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 (png) -> fps\n",
      "  fps -> Stream #0:0 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=6 deblock=1:1:1 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=0.40:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=5 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=15 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=10.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:0.60\n",
      "Output #0, mp4, to '/storage/aj/animatediff-cli-prompt-travel/stylize/real_base2-dance00025/fg_00_real_base2/2023-12-14_18-04-88/00_2023-12-14_18-05_.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 512x896, q=-1--1, 15 fps, 15360 tbn, 15 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=   16 fps=0.0 q=-1.0 Lsize=     543kB time=00:00:00.86 bitrate=5131.1kbits/s speed=4.06x    \n",
      "video:542kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.178042%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mframe I:1     Avg QP: 9.83  size: 50224\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mframe P:6     Avg QP: 9.27  size: 38936\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mframe B:9     Avg QP: 9.88  size: 30044\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mconsecutive B-frames: 25.0%  0.0% 18.8% 25.0% 31.2%  0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mmb I  I16..4: 34.1% 41.5% 24.4%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mmb P  I16..4: 28.2% 26.3% 10.0%  P16..4: 13.0% 10.7%  7.0%  0.0%  0.0%    skip: 4.8%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mmb B  I16..4:  5.6%  8.6%  5.1%  B16..8: 24.2% 15.8%  5.7%  direct:23.6%  skip:11.2%  L0:46.3% L1:35.7% BI:18.1%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0m8x8 transform intra:41.9% inter:59.7%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mcoded y,uvDC,uvAC intra: 53.7% 90.4% 88.9% inter: 35.3% 75.7% 58.0%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi16 v,h,dc,p: 83%  8%  3%  5%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 32%  8% 18%  5%  6% 11%  4% 10%  4%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 29% 11% 15%  7%  9% 12%  5%  8%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi8c dc,h,v,p: 39% 12% 37% 12%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mref P L0: 56.0%  8.3% 20.6%  8.9%  4.9%  1.3%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mref B L0: 67.9% 16.8% 10.4%  4.6%  0.3%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mref B L1: 95.5%  4.5%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mkb/s:4156.75\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Saved sample to                               \u001b]8;id=168261;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=252545;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1570\u001b\\\u001b[2m1570\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/storage/aj/animatediff-cli-prompt-travel/sty\u001b[0m \u001b[2m                \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35mlize/real_base2-dance00025/fg_00_real_base2/2\u001b[0m \u001b[2m                \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m023-12-14_18-04-88/\u001b[0m\u001b[95m00_2023-12-14_18-05_\u001b[0m       \u001b[2m                \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generation complete!                                \u001b]8;id=770760;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=837148;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Done, exiting\u001b[33m...\u001b[0m                                    \u001b]8;id=927747;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=314663;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#490\u001b\\\u001b[2m490\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Stylized results are output to                  \u001b]8;id=639472;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py\u001b\\\u001b[2mstylize.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=347586;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py#644\u001b\\\u001b[2m644\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/storage/aj/animatediff-cli-prompt-travel/styli\u001b[0m \u001b[2m              \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35mze/real_base2-dance00025/fg_00_real_base2/\u001b[0m\u001b[95m2023-\u001b[0m \u001b[2m              \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[95m12-14_18-04_00\u001b[0m                                  \u001b[2m              \u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!animatediff stylize create-config /storage/animatediff/animatediff-cli-prompt-travel/data/video/Download.mp4 --fps 25\n",
    "\n",
    "# [3] generate mask\n",
    "#!animatediff stylize create-mask /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-24T02-28-29-mumei-pyrite_v4\n",
    "\n",
    "# The foreground is output to the following directory (FG_STYLYZE_DIR)\n",
    "# STYLYZE_DIR/fg_00_timestamp_str\n",
    "# The background is output to the following directory (BG_STYLYZE_DIR)\n",
    "# STYLYZE_DIR/bg_timestamp_str\n",
    "\n",
    "#!cp -r /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004/00_controlnet_image/controlnet_tile/* /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004/fg_00_Santa/00_controlnet_image/animatediff_controlnet\n",
    "#!cp -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-06T14-46-41-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_tile/* /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-06T14-46-41-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_openpose\n",
    "#!cp -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-05T23-24-25-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_tile/* /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-05T23-24-25-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_openpose\n",
    "\n",
    "# [4] generate foreground\n",
    "\n",
    "!animatediff stylize generate /storage/aj/animatediff-cli-prompt-travel/stylize/real_base2-dance00025/fg_00_real_base2 -L 16\n",
    "#!rm -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T14-08-07-e-majicmixrealistic_betterv2v25/2023*\n",
    "#!animatediff stylize generate /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T14-08-07-e-majicmixrealistic_betterv2v25 -L 16\n",
    "\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-05T23-24-25-sample-majicmixrealistic_betterv2v25/2023-11-07T17-35-46_00/00-341774366206100 -W 768\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-07T16-42-20-sample-majicmixrealistic_betterv2v25/2023-11-07T18-24-43_00/00-341774366206100 -W 768\n",
    "\n",
    "#!animatediff stylize create-region /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-07T16-42-20-sample-majicmixrealistic_betterv2v25\n",
    "\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T06-42-55-dance22-majicmixrealistic_betterv2v25/2023-11-09T09-41-58_00/00-341774366206100 -W 768\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T06-42-55-dance22-majicmixrealistic_betterv2v25/2023-11-09T09-41-58_01/00-341774366206100 -W 1024\n",
    "\n",
    "#!animatediff refine /storage/aj/animatediff-cli-prompt-travel/stylize/jjj-dance33/fg_00_jjj/2023-11-29_01-33_00/00-2023-11-29_01-34 -W 512\n",
    "\n",
    "\n",
    "#!rm -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/bg_work/*\n",
    "#!for file in /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-08T15-15-10-d10_msk-majicmixrealistic_betterv2v25/bg_2023-11-08T15-23-08/00_controlnet_image/controlnet_tile/*; do cp \"/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/bg/90000001.png\" \"/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/bg_work/$(basename \"$file\")\"; done\n",
    "#!animatediff stylize composite /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
