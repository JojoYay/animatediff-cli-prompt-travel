{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262688db-ad61-4577-bd8c-69e0d21e4cc2",
   "metadata": {},
   "source": [
    "# AnimateDiff-V2V-GUI\n",
    "\n",
    "I have forked AnimateDiff prompt travel to add front GUI\n",
    "to do that, actually I have updated all those folder names etc\n",
    "\n",
    "What you need is slitely different.\n",
    "I'm keep syncing the repo so far and hopefully those basic function can also be available.\n",
    "(But result folder structure is a bit different)\n",
    "\n",
    "\n",
    "### How is it?\n",
    "\n",
    "[AnimateDiff with prompt travel](https://github.com/s9roll7/animatediff-cli-prompt-travel) + [ControlNet](https://github.com/lllyasviel/ControlNet) + [IP-Adapter](https://github.com/tencent-ailab/IP-Adapter)\n",
    "And now you have GUI\n",
    "\n",
    "### How to install\n",
    "\n",
    "Python 3.10 and git client must be installed.\n",
    "\n",
    "git clone the repo.\n",
    "\n",
    "```shell\n",
    "\n",
    "git clone https://github.com/JojoYay/animatediff-cli-prompt-travel\n",
    "cd animatediff-cli-prompt-travel\n",
    "py -3.10 -m venv venv\n",
    "venv\\Scripts\\activate.bat\n",
    "set PYTHONUTF8=1\n",
    "python -m pip install --upgrade pip\n",
    "# Torch installation must be modified to suit the environment. (https://pytorch.org/get-started/locally/)\n",
    "python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "python -m pip install -e .\n",
    "\n",
    "```\n",
    "### Model, Motion Module, Lora prep\n",
    "You need to prepare Model, Motion Module, Lora and place them below\n",
    "\n",
    "model : /animatediff-cli-prompt-travel/data/sd_models\n",
    "LoRA : /animatediff-cli-prompt-travel/data/lora\n",
    "model : /animatediff-cli-prompt-travel/data/motion_modules\n",
    "\n",
    "when you start, you can see the URL in console.\n",
    "Click and enjoy making video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102236ff-f99a-44cf-bef2-c33492143044",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T12:38:35.642565Z",
     "iopub.status.busy": "2023-12-19T12:38:35.642289Z",
     "iopub.status.idle": "2023-12-19T12:38:35.649769Z",
     "shell.execute_reply": "2023-12-19T12:38:35.648990Z",
     "shell.execute_reply.started": "2023-12-19T12:38:35.642543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/storage/aj/animatediff-cli-prompt-travel', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/storage/aj/animatediff-cli-prompt-travel/src']\n"
     ]
    }
   ],
   "source": [
    "#!rm -r /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/real_base2-dance00004\n",
    "\n",
    "\n",
    "\n",
    "#import onnxruntime as ort\n",
    "#ort.get_device()\n",
    "#ort.get_available_providers()\n",
    "#!pip uninstall onnxruntime onnxruntime-gpu onnxruntime_gpu -y\n",
    "#!pip install onnxruntime_gpu\n",
    "\n",
    "import sys\n",
    "sys.path.append('/storage/aj/animatediff-cli-prompt-travel/src')\n",
    "print(sys.path)\n",
    "\n",
    "#!cp /notebooks/storage/aj/animatediff-cli-prompt-travel/config/test/* /notebooks/storage/aj/animatediff-cli-prompt-travel/config/test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8a0f9-47d4-4261-b356-86609654cee7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T15:11:07.496416Z",
     "iopub.status.busy": "2023-12-19T15:11:07.495641Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diffuser_ver='0.23.0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://3de434e035bcce1a0c.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3de434e035bcce1a0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TikTok] Extracting URL: https://www.tiktok.com/@namdareun_/video/7309769737024605447\n",
      "[TikTok] 7309769737024605447: Downloading video feed\n",
      "[info] 7309769737024605447: Downloading 1 format(s): bytevc1_1080p_1730203-2\n",
      "[download] data/video/dance00032.mp4 has already been downloaded\n",
      "[download] 100% of    7.72MiB\n",
      "inp_posi(masterpiece, best quality, detailed), 1girl, solo, indoors, changing room, arms behind head, cowboy shot, souryuu asuka langley, interface headset, red bodysuit\n",
      "inp_lora1lora/sd/Asuka.safetensors\n",
      "inp_lora1_step1\n",
      "neg_prompt(low quality, worst quality:1.4), signature, bad-hands-5, EasyNegativeV2, multicolored bodysuit\n",
      "<PIL.Image.Image image mode=RGB size=1024x1536 at 0x7F95A7739FC0>\n",
      "lora/sd/Asuka.safetensors\n",
      "########################################################\n",
      "Image saved successfully to /storage/aj/animatediff-cli-prompt-travel/stylize/dance00032/00_ipadapter/0.png\n",
      "########################################################\n",
      "video1: data/video/dance00032.mp4\n",
      "stylize_dir:stylize/dance00032\n",
      "stylize_fg_dir:stylize/dance00032/fg_00_dance00032\n",
      "config already exists. skip create-config\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "diffuser_ver='0.23.0'\n",
      "Using generation config: stylize/dance00032/fg_00_dance00032/prompt.json\n",
      "is_sdxl=False\n",
      "is_v2=True\n",
      "Using base model: runwayml/stable-diffusion-v1-5\n",
      "Will save outputs to ./stylize/dance00032/fg_00_dance00032/2023-12-19_23-13-88\n",
      "Preprocessing images (animatediff_controlnet): 100%|█| 121/121 [00:01<00:00, 97.\n",
      "Preprocessing images (controlnet_openpose):   0%|       | 0/121 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "Preprocessing images (controlnet_openpose): 100%|█| 121/121 [02:35<00:00,  1.28s\n",
      "Preprocessing images (controlnet_depth):   0%|          | 0/121 [00:00<?, ?it/s]Loading depth_midas\n",
      "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name vit_base_resnet50_384 to current vit_base_r50_s16_384.orig_in21k_ft_in1k.\n",
      "  model = create_fn(\n",
      "Preprocessing images (controlnet_depth): 100%|█| 121/121 [00:22<00:00,  5.38it/s\n",
      "Checking motion module...\n",
      "Loading tokenizer...\n",
      "Loading text encoder...\n",
      "Loading VAE...\n",
      "Loading UNet...\n",
      "Loaded 453.20928M-parameter motion module\n",
      "gradual_latent_hires_fix enable\n",
      "model_config.scheduler=<DiffusionScheduler.k_dpmpp_2m: 'k_dpmpp_2m'>\n",
      "If you are forced to exit with an error, change to euler_a or lcm\n",
      "Using scheduler \"k_dpmpp_2m\" (DPMSolverMultistepScheduler)\n",
      "Loading weights from /storage/aj/animatediff-cli-prompt-travel/data/sd_models/Stable-diffution/sudachi_v10.safetensors\n",
      "Merging weights into UNet...\n",
      "Creating AnimationPipeline...\n",
      "lora_path=PosixPath('/storage/aj/animatediff-cli-prompt-travel/data/models/lcm_lora/sd15/pytorch_lora_weights.safetensors')\n",
      "create LoRA network from weights\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc2 (not found in modules_dim)\n",
      "create LoRA for Text Encoder: 0 modules.\n",
      "skipped 72 modules because of missing weight for text encoder.\n",
      "create LoRA for U-Net: 278 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "No TI embeddings found\n",
      "loading c='animatediff_controlnet' model\n",
      "loading c='controlnet_openpose' model\n",
      "loading c='controlnet_depth' model\n",
      "Sending pipeline to device \"cuda\"\n",
      "Selected data types: unet_dtype=torch.float16, tenc_dtype=torch.float16, vae_dtype=torch.bfloat16\n",
      "Using channels_last memory format for UNet and VAE\n",
      "Preprocessing images (ip_adapter): 100%|██████████| 1/1 [00:00<00:00, 17.21it/s]\n",
      "c='animatediff_controlnet' / []\n",
      "c='controlnet_openpose' / []\n",
      "c='controlnet_depth' / []\n",
      "Saving prompt config to output directory\n",
      "Initialization complete!\n",
      "Generating 1 animations\n",
      "Running generation 1 of 1\n",
      "Generation seed: 74699447653086840\n",
      "len( region_condi_list )=1\n",
      "len( region_list )=1\n",
      "apply_lcm_lora=True\n",
      "controlnet_for_region=True\n",
      "multi_uncond_mode=True\n",
      "unet_batch_size=1\n",
      "prompt_encoder.get_condi_size()=2\n",
      " 71%|█████████████████████████████            | 109/154 [07:29<06:22,  8.49s/it]"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from animatediff.execute import execute\n",
    "from animatediff.front_utils import (get_schedulers, validate_inputs, getNow, download_video, create_file_list,\n",
    "                                    find_safetensor_files, find_last_folder_and_mp4_file, find_next_available_number,\n",
    "                                    find_and_get_composite_video, load_video_name, get_last_sorted_subfolder,\n",
    "                                    create_config_by_gui, get_config_path, update_config, change_ip, change_ad, change_op,\n",
    "                                    change_dp, change_la)\n",
    "from animatediff.settings import ModelConfig, get_model_config\n",
    "from animatediff.video_utils import create_video\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Define the function signature\n",
    "def execute_wrapper(\n",
    "      url: str, \n",
    "      inp_model: str, inp_mm: str,\n",
    "      inp_sche: str, inp_step: int, inp_cfg: float, \n",
    "      inp_posi: str, inp_neg: str, \n",
    "      inp_lora1: str, inp_lora1_step: float,\n",
    "      inp_lora2: str, inp_lora2_step: float,\n",
    "      inp_lora3: str, inp_lora3_step: float,\n",
    "      inp_lora4: str, inp_lora4_step: float,\n",
    "      ip_ch: bool, ip_image: str, ip_scale: float, ip_type: str,\n",
    "      ad_ch: bool, ad_scale: float, op_ch: bool, op_scale: float,\n",
    "      dp_ch: bool, dp_scale:float, la_ch: bool, la_scale: float,\n",
    "      delete_if_exists: bool, is_test: bool, is_refine: bool,\n",
    "      progress=gr.Progress(track_tqdm=True)):\n",
    "    \n",
    "    yield 'generation Initiated...', None, None, None, None, gr.Button(\"Generating...\", scale=1, interactive=False)\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    time_str = getNow()\n",
    "    validate_inputs(url)\n",
    "\n",
    "    bg_config = None\n",
    "    save_folder = 'data/video'\n",
    "    saved_file = download_video(url, save_folder)\n",
    "    video_name=saved_file.rsplit('.', 1)[0].rsplit('/notebooks', 1)[-1].rsplit('/', 1)[-1]\n",
    "    stylize_dir= Path('/storage/aj/animatediff-cli-prompt-travel/stylize/' + video_name)\n",
    "    create_config_by_gui(\n",
    "        now_str=time_str,\n",
    "        video = saved_file,\n",
    "        stylize_dir = stylize_dir, \n",
    "        model=inp_model, motion_module=inp_mm, \n",
    "        scheduler=inp_sche, step=inp_step, cfg=inp_cfg, \n",
    "        head_prompt=inp_posi, neg_prompt=inp_neg,\n",
    "        inp_lora1=inp_lora1, inp_lora1_step=inp_lora1_step,\n",
    "        inp_lora2=inp_lora2, inp_lora2_step=inp_lora2_step,\n",
    "        inp_lora3=inp_lora3, inp_lora3_step=inp_lora3_step,\n",
    "        inp_lora4=inp_lora4, inp_lora4_step=inp_lora4_step,\n",
    "        ip_ch=ip_ch, ip_image=ip_image, ip_scale=ip_scale, ip_type=ip_type,\n",
    "        ad_ch=ad_ch, ad_scale=ad_scale, op_ch=op_ch, op_scale=op_scale,\n",
    "        dp_ch=dp_ch, dp_scale=dp_scale, la_ch=la_ch, la_scale=la_scale,\n",
    "    )\n",
    "\n",
    "    yield from execute_impl(now_str=time_str,video=saved_file, delete_if_exists=delete_if_exists, is_test=is_test, is_refine=is_refine, bg_config=bg_config)\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(f\"実行時間: {execution_time}秒\")\n",
    "\n",
    "def execute_impl(now_str:str,\n",
    "                 video: str, \n",
    "                 # config: Path, \n",
    "                 delete_if_exists: bool, \n",
    "                 is_test: bool,\n",
    "                 is_refine: bool, \n",
    "                 bg_config: str):\n",
    "    if video.startswith(\"/notebooks\"):\n",
    "        video = video[len(\"/notebooks\"):]\n",
    "    if bg_config is not None:\n",
    "        if bg_config.startswith(\"/notebooks\"):\n",
    "            bg_config = bg_config[len(\"/notebooks\"):]\n",
    "    print(f\"video1: {video}\")\n",
    "    yield 'generating config...', video, None, None, None, gr.Button(\"Generating...\", scale=1, interactive=False)\n",
    "        \n",
    "    video_name=video.rsplit('.', 1)[0].rsplit('/notebooks', 1)[-1].rsplit('/', 1)[-1]\n",
    "\n",
    "    stylize_dir='stylize/' + video_name\n",
    "    stylize_fg_dir = stylize_dir + '/fg_00_'+video_name\n",
    "    stylize_fg_dir = Path(stylize_fg_dir)\n",
    "    stylize_bg_dir = stylize_dir + '/bg_'+video_name\n",
    "    stylize_bg_dir = Path(stylize_bg_dir)\n",
    "    stylize_dir = Path(stylize_dir)\n",
    "    print(f\"stylize_dir:{stylize_dir}\")\n",
    "    print(f\"stylize_fg_dir:{stylize_fg_dir}\")\n",
    "\n",
    "    if bg_config is not None:\n",
    "        bg_config = Path(bg_config)\n",
    "        bg_model_config: ModelConfig = get_model_config(bg_config)\n",
    "\n",
    "    if stylize_dir.exists() and not delete_if_exists:\n",
    "        print(f\"config already exists. skip create-config\")\n",
    "    else:\n",
    "        if stylize_dir.exists():\n",
    "            print(f\"Delete folder and create again\")\n",
    "            shutil.rmtree(stylize_dir)\n",
    "        # create_config(org_movie=video,config_org=config,fps=15)\n",
    "        !animatediff stylize create-config {video}\n",
    "        # create_mask(stylize_dir=stylize_dir, bg_config=bg_config, no_crop=True)\n",
    "        !animatediff stylize create-mask {stylize_dir} -nc\n",
    "\n",
    "    update_config(now_str, stylize_dir, stylize_fg_dir)\n",
    "    config = get_config_path(now_str)\n",
    "    model_config: ModelConfig = get_model_config(config)       \n",
    "        \n",
    "    yield 'generating fg bg video...', video, None, None, None, gr.Button(\"Generating...\", scale=1, interactive=False)\n",
    "\n",
    "    if is_test:\n",
    "  #      generate(stylize_dir=stylize_fg_dir, length=16)\n",
    "        !animatediff stylize generate {stylize_fg_dir} -L 16\n",
    "        if bg_config is not None:\n",
    "            # generate(stylize_dir=stylize_bg_dir, length=16)\n",
    "            !animatediff stylize generate {stylize_bg_dir} -L 16\n",
    "\n",
    "    else:\n",
    "        # generate(stylize_dir=stylize_fg_dir)\n",
    "        !animatediff stylize generate {stylize_fg_dir}\n",
    "        if bg_config is not None:\n",
    "            # generate(stylize_dir=stylize_bg_dir)\n",
    "            !animatediff stylize generate {stylize_bg_dir}\n",
    "            \n",
    "    video2 = find_last_folder_and_mp4_file(stylize_fg_dir)\n",
    "    print(f\"video2: {video2}\")\n",
    "\n",
    "    if is_refine:\n",
    "        yield 'refining fg video', video, video2, None, None, gr.Button(\"Generating...\", scale=1, interactive=False)\n",
    "\n",
    "        result_dir = get_last_sorted_subfolder(get_last_sorted_subfolder(stylize_fg_dir))\n",
    "#        refine(frames_dir=result_dir, out_dir=stylize_fg_dir, config_path=config, width=768)\n",
    "        !animatediff refine {result_dir} -o {stylize_fg_dir} -c {config} -W 768\n",
    "        video3 = find_last_folder_and_mp4_file(get_last_sorted_subfolder(stylize_fg_dir))\n",
    "        print(f\"video3: {video3}\")\n",
    "        yield 'compositing video', video, video2, video3, None, gr.Button(\"Generate Video\", scale=1, interactive=False)\n",
    "        fg_result = get_last_sorted_subfolder(get_last_sorted_subfolder(get_last_sorted_subfolder(stylize_fg_dir)))\n",
    "\n",
    "    else:\n",
    "        yield 'composite video', video, video2, None, None, gr.Button(\"Generating...\", scale=1, interactive=False)\n",
    "        fg_result = get_last_sorted_subfolder(get_last_sorted_subfolder(stylize_fg_dir))\n",
    "        video3 = None\n",
    "\n",
    "    bg_result = get_last_sorted_subfolder(stylize_bg_dir)\n",
    "\n",
    "    print(f\"fg_dir:{fg_result}\")\n",
    "    if bg_config is not None:\n",
    "        print(f\"bg_dir: {bg_result}\")\n",
    "    else:\n",
    "        print(f\"bg_dir: {stylize_bg_dir/'00_img2img'}\")\n",
    "\n",
    "    if bg_config is not None:\n",
    "        # final_video_dir = composite(stylize_dir=stylize_dir, bg_dir=bg_result, fg_dir=fg_result)\n",
    "        !animatediff stylize composite {stylize_dir} -bg {bg_result} -fg {fg_result}  \n",
    "    else:\n",
    "        bg_result = stylize_bg_dir/'00_img2img'\n",
    "        # final_video_dir = composite(stylize_dir=stylize_dir, bg_dir=stylize_bg_dir/'00_img2img', fg_dir=fg_result)\n",
    "        !animatediff stylize composite {stylize_dir} -bg {bg_result} -fg {fg_result}\n",
    "\n",
    "    fg_result = find_and_get_composite_video(stylize_dir)\n",
    "    print(f\"final_video_dir: {fg_result}\")\n",
    "\n",
    "    final_dir = os.path.dirname(fg_result)\n",
    "    new_file_path = os.path.join(final_dir,  video_name + \".mp4\")\n",
    "    \n",
    "#    final_video_dir: stylize/dance00023/cp_2023-12-18_08-09/composite2023-12-18_08-09-41\n",
    "\n",
    "    try:\n",
    "        create_video(video, fg_result, new_file_path)\n",
    "        print(f\"new_file_path:{new_file_path}\")\n",
    "        yield 'video is ready!', video, video2, video3, new_file_path, gr.Button(\"Generate Video\", scale=1, interactive=True)\n",
    "    except Exception as e:\n",
    "        print(f\"error:{e}\")\n",
    "        yield 'video is ready!(no music added)', video, video2, video3, fg_result, gr.Button(\"Generate Video\", scale=1, interactive=True)\n",
    "        \n",
    "def launch():\n",
    "    result_list = create_file_list(\"config/fix\")\n",
    "    safetensor_files = find_safetensor_files(\"data/sd_models\")\n",
    "    lora_files = find_safetensor_files(\"data/lora\")\n",
    "    mm_files = find_safetensor_files(\"data/motion_modules\")\n",
    "    schedulers = get_schedulers()\n",
    "    ip_choice = [\"full_face\", \"plus_face\", \"plus\", \"light\"]\n",
    "\n",
    "    with gr.Blocks() as iface:\n",
    "        with gr.Row():\n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                # AnimateDiff-V2V-GUI\n",
    "                \"\"\", scale=8)\n",
    "            btn = gr.Button(\"Generate Video\", scale=1)\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                with gr.Group():\n",
    "                    url = gr.Textbox(lines=1, value=\"https://www.tiktok.com/@ai_hinahina/video/7313863412541361426\", placeholder=\"https://www.tiktok.com/@ai_hinahina/video/7313863412541361426\", label=\"URL\")\n",
    "                    with gr.Group():\n",
    "                        with gr.Row():\n",
    "                            inp_model = gr.Dropdown(choices=safetensor_files, label=\"Model\")\n",
    "                            inp_mm = gr.Dropdown(choices=mm_files, label=\"Motion Module\")\n",
    "                    with gr.Group():\n",
    "                        with gr.Row():\n",
    "                            inp_sche = gr.Dropdown(choices=schedulers, label=\"Sampling Method\")\n",
    "                            inp_step = gr.Slider(minimum=1, maximum=20, step=1, value=9, label=\"Sampling Steps\")\n",
    "                            inp_cfg = gr.Slider(minimum=0.1, maximum=10, step=0.1,  value=2.3, label=\"CFG Scale\")\n",
    "                    inp_posi = gr.Textbox(lines=2, value=\"1girl, beautiful\", placeholder=\"1girl, beautiful\", label=\"Positive Prompt\")\n",
    "                    inp_neg = gr.Textbox(lines=2, value=\"low quality, low res,\", placeholder=\"low quality, low res,\", label=\"Negative Prompt\")\n",
    "                    with gr.Accordion(\"LoRAs\", open=False):\n",
    "                        with gr.Group():\n",
    "                            with gr.Row():\n",
    "                                inp_lora1 = gr.Dropdown(choices=lora_files, label=\"Lora1\", scale=3)\n",
    "                                inp_lora1_step = gr.Slider(minimum=0.1, maximum=3, step=0.1, value=1.0, label=\"LoRA1 Scale\", scale=1)\n",
    "                        with gr.Group():\n",
    "                            with gr.Row():\n",
    "                                inp_lora2 = gr.Dropdown(choices=lora_files, label=\"Lora2\", scale=3)\n",
    "                                inp_lora2_step = gr.Slider(minimum=0.1, maximum=3, step=0.1, value=1.0, label=\"LoRA2 Scale\", scale=1)\n",
    "                        with gr.Group():\n",
    "                            with gr.Row():\n",
    "                                inp_lora3 = gr.Dropdown(choices=lora_files, label=\"Lora3\", scale=3)\n",
    "                                inp_lora3_step = gr.Slider(minimum=0.1, maximum=3, step=0.1, value=1.0, label=\"LoRA3 Scale\", scale=1)\n",
    "                        with gr.Group():\n",
    "                            with gr.Row():\n",
    "                                inp_lora4 = gr.Dropdown(choices=lora_files, label=\"Lora4\", scale=3)\n",
    "                                inp_lora4_step = gr.Slider(minimum=0.1, maximum=3, step=0.1, value=1.0, label=\"LoRA4 Scale\", scale=1)\n",
    "\n",
    "                    with gr.Accordion(\"ControlNet\", open=False):\n",
    "                    # with gr.Group():\n",
    "                        ip_ch = gr.Checkbox(label=\"IPAdapter\", value=False)\n",
    "                        ip_image = gr.Image(height=256, type=\"pil\", interactive=False)\n",
    "                        # ip_upload = gr.UploadButton(label='Click to uplaod Image', file_types=[\"image\"], file_count=\"single\")\n",
    "                        with gr.Row():\n",
    "                            ip_scale = gr.Slider(minimum=0, maximum=3, step=0.1, value=0.5, label=\"IPAdapter scale\", interactive=False)\n",
    "                            ip_type = gr.Radio(choices=ip_choice, label=\"IPAdapter Type\", value=\"plus_face\", interactive=False)\n",
    "\n",
    "                    # with gr.Group():\n",
    "                        with gr.Row():\n",
    "                            ad_ch = gr.Checkbox(label=\"AimateDiff Controlnet\", value=True)\n",
    "                            ad_scale = gr.Slider(minimum=0, maximum=3,  step=0.1, value=0.8, label=\"AnimateDiff Controlnet scale\")\n",
    "                    # with gr.Group():\n",
    "                        with gr.Row():\n",
    "                            op_ch = gr.Checkbox(label=\"Open Pose\", value=True)\n",
    "                            op_scale = gr.Slider(minimum=0, maximum=3,  step=0.1, value=1.0, label=\"Open Pose scale\")\n",
    "                    # with gr.Group():\n",
    "                        with gr.Row():\n",
    "                            dp_ch = gr.Checkbox(label=\"Depth\", value=False)\n",
    "                            dp_scale = gr.Slider(minimum=0, maximum=3,  step=0.1, value=1.0, label=\"Depth scale\", interactive=False)\n",
    "                    # with gr.Group():\n",
    "                        with gr.Row():\n",
    "                            la_ch = gr.Checkbox(label=\"Lineart\", value=False)\n",
    "                            la_scale = gr.Slider(minimum=0, maximum=3,  step=0.1, value=1.0, label=\"Lineart scale\", interactive=False)\n",
    "                                \n",
    "                 #   inp2 = gr.Dropdown(choices=result_list, info=\"please select\", label=\"Config\")\n",
    "                    with gr.Row():\n",
    "                        delete_if_exists = gr.Checkbox(label=\"Delete cache\")\n",
    "                        test_run = gr.Checkbox(label=\"Test Run\", value=True)\n",
    "                        refine = gr.Checkbox(label=\"Refine\")\n",
    "                    \n",
    "\n",
    "            with gr.Column():\n",
    "                with gr.Group():\n",
    "                    o_status = gr.Label(value=\"Not Started Yet\", label=\"Status\")\n",
    "                    with gr.Row():\n",
    "                        o_video1 = gr.Video(width=256, label=\"Original Video\", show_share_button=True)\n",
    "                        o_video2 = gr.Video(width=256, label=\"Front Video\", show_share_button=True)\n",
    "                    with gr.Row():\n",
    "                        o_video3 = gr.Video(width=256, label=\"Refined Front Video\", show_share_button=True)\n",
    "                        o_video4 = gr.Video(width=256, label=\"Generated Video\", show_share_button=True)\n",
    "        \n",
    "        btn.click(fn=execute_wrapper,\n",
    "                  inputs=[url, \n",
    "                          inp_model, inp_mm,\n",
    "                          inp_sche, inp_step, inp_cfg, \n",
    "                          inp_posi, inp_neg, \n",
    "                          inp_lora1, inp_lora1_step,\n",
    "                          inp_lora2, inp_lora2_step,\n",
    "                          inp_lora3, inp_lora3_step,\n",
    "                          inp_lora4, inp_lora4_step,\n",
    "                          ip_ch, ip_image, ip_scale, ip_type,\n",
    "                          ad_ch, ad_scale, op_ch, op_scale,\n",
    "                          dp_ch, dp_scale, la_ch, la_scale,\n",
    "                          delete_if_exists, test_run, refine],\n",
    "                  outputs=[o_status, o_video1, o_video2, o_video3, o_video4, btn])\n",
    "\n",
    "        ip_ch.change(fn=change_ip, inputs=[ip_ch], outputs=[ip_ch, ip_image, ip_scale, ip_type])        \n",
    "        ad_ch.change(fn=change_ad, inputs=[ad_ch], outputs=[ad_ch, ad_scale])\n",
    "        op_ch.change(fn=change_op, inputs=[op_ch], outputs=[op_ch, op_scale])\n",
    "        dp_ch.change(fn=change_dp, inputs=[dp_ch], outputs=[dp_ch, dp_scale])\n",
    "        la_ch.change(fn=change_la, inputs=[la_ch], outputs=[la_ch, la_scale])\n",
    "\n",
    "        \n",
    "    iface.queue()\n",
    "    iface.launch(share=True)\n",
    "\n",
    "    while True:\n",
    "        pass\n",
    "\n",
    "launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "791a49c2-032b-49f3-8536-2d8f9fed3cc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T12:38:42.694066Z",
     "iopub.status.busy": "2023-12-19T12:38:42.693795Z",
     "iopub.status.idle": "2023-12-19T12:41:23.191403Z",
     "shell.execute_reply": "2023-12-19T12:41:23.190481Z",
     "shell.execute_reply.started": "2023-12-19T12:38:42.694044Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/notebooks/storage': File exists\n",
      "ln: failed to create symbolic link '/notebooks/tmp': File exists\n",
      "Stored 'repo_dir' (str)\n",
      "Stored 'repo_storage_dir_aj' (str)\n",
      "/storage/aj/animatediff-cli-prompt-travel\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.0.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0.1\n",
      "    Uninstalling pip-23.0.1:\n",
      "      Successfully uninstalled pip-23.0.1\n",
      "Successfully installed pip-23.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping onnxruntime_gpu as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting onnxruntime_gpu\n",
      "  Downloading onnxruntime_gpu-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting coloredlogs (from onnxruntime_gpu)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime_gpu) (1.12)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime_gpu) (1.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime_gpu) (23.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime_gpu) (3.19.6)\n",
      "Collecting sympy (from onnxruntime_gpu)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime_gpu)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19 (from sympy->onnxruntime_gpu)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime_gpu-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.1/157.1 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:04\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, humanfriendly, coloredlogs, onnxruntime_gpu\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 mpmath-1.3.0 onnxruntime_gpu-1.16.3 sympy-1.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///storage/aj/animatediff-cli-prompt-travel\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting accelerate>=0.20.3 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.4.3)\n",
      "Collecting cmake>=3.25.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting diffusers==0.23.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading diffusers-0.23.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting einops>=0.6.1 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gdown>=4.6.6 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
      "Collecting ninja>=1.11.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.23.4)\n",
      "Collecting omegaconf>=2.3.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pillow<10.0.0,>=9.4.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading Pillow-9.5.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: rich<14.0.0,>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (13.3.2)\n",
      "Collecting safetensors>=0.3.1 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting sentencepiece>=0.1.99 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: shellingham<2.0.0,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.5.0.post1)\n",
      "Collecting torch<2.2.0,>=2.1.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.12.1+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.13.1+cu116)\n",
      "Collecting transformers<4.35.0,>=4.30.2 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<1.0.0,>=0.9.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting controlnet-aux (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading controlnet_aux-0.0.7.tar.gz (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.4/202.4 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (3.6.1)\n",
      "Collecting ffmpeg-python>=0.2.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting mediapipe (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting xformers>=0.0.22.post7 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.26.15)\n",
      "Collecting yt-dlp (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading yt_dlp-2023.11.16-py2.py3-none-any.whl.metadata (160 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.5/160.5 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gradio (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio-4.10.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.16.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.5.0)\n",
      "Collecting segment-anything-hq==0.3 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading segment_anything_hq-0.3-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting groundingdino-py==0.4.0 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading groundingdino-py-0.4.0.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (3.1.31)\n",
      "Collecting rembg[gpu] (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading rembg-2.0.53-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting moviepy (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting importlib-metadata (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading importlib_metadata-7.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.9.0)\n",
      "Collecting huggingface-hub>=0.13.2 (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.28.2)\n",
      "Collecting addict (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Collecting yapf (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading yapf-0.40.2-py3-none-any.whl.metadata (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading timm-0.9.12-py3-none-any.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.6.0.66)\n",
      "Collecting supervision==0.6.0 (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading supervision-0.6.0-py3-none-any.whl (31 kB)\n",
      "Collecting pycocotools (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev267+g7b84f6b.d20231219) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.4.1)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from ffmpeg-python>=0.2.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.18.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.64.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.11.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.3.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.5.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2023.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchvision (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers<4.35.0,>=4.30.2->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (8.1.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.9.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.19.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.0.10)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fastapi (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading fastapi-0.105.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.7.3 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting httpx (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.12.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.1.2)\n",
      "Collecting orjson~=3.0 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gradio (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio-4.9.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.2 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio_client-0.7.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting gradio (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio-4.8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.1 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio_client-0.7.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio-4.7.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.7.0 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio_client-0.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gradio (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.4.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: pip is still looking at multiple versions of gradio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading gradio-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.1.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.1.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.1.0-py3-none-any.whl.metadata (17 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading gradio-4.0.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.0.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-4.0.0-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading gradio-3.50.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting gradio-client==0.6.1 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading gradio_client-0.6.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydub (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting semantic-version~=2.0 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->animatediff==0.1.dev267+g7b84f6b.d20231219) (2022.7.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.4.0)\n",
      "Collecting attrs>=19.1.0 (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=2.0 (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting opencv-contrib-python (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading opencv_contrib_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.19.6)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
      "Collecting decorator<5.0,>=4.0.2 (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting proglog<=1.0.0 (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.26.0)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu->animatediff==0.1.dev267+g7b84f6b.d20231219) (15.0.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.17.3)\n",
      "Collecting onnxruntime (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting opencv-python-headless (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading opencv_python_headless-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting pooch (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading pooch-1.8.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pymatting (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading PyMatting-1.1.12-py3-none-any.whl.metadata (7.4 kB)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting mutagen (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading mutagen-1.47.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting pycryptodomex (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219) (2019.11.28)\n",
      "Collecting requests (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3<2 (from animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting brotli (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.12.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.0.0)\n",
      "Collecting fsspec (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219) (67.6.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.19.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14.0.0,>=13.0.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.8)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.15.1)\n",
      "Collecting huggingface-hub>=0.13.2 (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime-gpu->animatediff==0.1.dev267+g7b84f6b.d20231219) (10.0)\n",
      "Collecting anyio<4.0.0,>=3.7.1 (from fastapi->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typing-extensions>=4.2.0 (from pydantic<2.0.0,>=1.10.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting httpcore==1.* (from httpx->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.3.0)\n",
      "Collecting zipp>=0.5 (from importlib-metadata->diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading zipp-3.17.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.1.1)\n",
      "Collecting numba!=0.49.0 (from pymatting->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "INFO: pip is looking at multiple versions of requests[socks] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.7.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (2023.2.28)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.4.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.3.0)\n",
      "Collecting platformdirs>=2.5.0 (from pooch->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading platformdirs-4.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tomli>=2.0.1 (from yapf->groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting exceptiongroup (from anyio<4.0.0,>=3.7.1->fastapi->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.21)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba!=0.49.0->pymatting->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219)\n",
      "  Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Downloading diffusers-0.23.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading segment_anything_hq-0.3-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.16.2-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (213.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchaudio-2.1.2-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yt_dlp-2023.11.16-py2.py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m117.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-7.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading mutagen-1.47.0-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_contrib_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pooch-1.8.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycocotools-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.2/426.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodomex-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMatting-1.1.12-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rembg-2.0.53-py3-none-any.whl (32 kB)\n",
      "Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yapf-0.40.2-py3-none-any.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-4.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: animatediff, groundingdino-py, antlr4-python3-runtime, controlnet-aux, moviepy, ffmpy\n",
      "  Building editable for animatediff (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for animatediff: filename=animatediff-0.1.dev267+g7b84f6b.d20231219-0.editable-py3-none-any.whl size=6608 sha256=1a3af3a7c915a5c039d55e57d881f44849a035c339dec854a0a49217d4797f34\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ty81kkk5/wheels/a3/53/5e/a55600e8a54c8af9f77a7cefdd48769ca283a356ff22cc1550\n",
      "  Building wheel for groundingdino-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for groundingdino-py: filename=groundingdino_py-0.4.0-py2.py3-none-any.whl size=88739 sha256=881b6bc3957f30aa3353efa17ba58f838aec56a0dd4ba68040e0257246bde19e\n",
      "  Stored in directory: /root/.cache/pip/wheels/72/25/30/97b491abad279d329c62bef1e91bc56bf2fd40b22281068e1d\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=9b82f59a32e4ab6dc5bd360151c1faad80a874e517d5b57a6cbc9089f83a3660\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for controlnet-aux (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for controlnet-aux: filename=controlnet_aux-0.0.7-py3-none-any.whl size=274341 sha256=146e92c2cf1b1bac90e03c14a8b8e0efd2b020b184b65311a745bbd8d459134a\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/3e/93/6678b4c0bc2ec31d53409b25d4189cbb08bae843e8b2b78e52\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110728 sha256=1c9710d4d0e95ffd92eefe7ebb555d9ce75482814b01759455481f8610bf605f\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/32/2d/e10123bd88fbfc02fed53cc18c80a171d3c87479ed845fa7c1\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=1450ecaf3011cad67185973a3d7bf75dc3c2b92ec1f6e67e27e8c89af7d70fda\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
      "Successfully built animatediff groundingdino-py antlr4-python3-runtime controlnet-aux moviepy ffmpy\n",
      "Installing collected packages: sentencepiece, pydub, ninja, flatbuffers, ffmpy, cmake, brotli, antlr4-python3-runtime, addict, zipp, websockets, urllib3, typing-extensions, triton, tomli, semantic-version, safetensors, python-multipart, pycryptodomex, proglog, platformdirs, pillow, orjson, opencv-python-headless, opencv-contrib-python, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mutagen, llvmlite, imageio_ffmpeg, h11, fsspec, ffmpeg-python, exceptiongroup, einops, decorator, attrs, aiofiles, uvicorn, typer, sounddevice, requests, pydantic, onnxruntime, nvidia-cusparse-cu12, nvidia-cudnn-cu12, numba, importlib-metadata, httpcore, anyio, yt-dlp, yapf, supervision, starlette, pymatting, pycocotools, pooch, nvidia-cusolver-cu12, moviepy, mediapipe, huggingface-hub, httpx, altair, torch, tokenizers, rembg, gradio-client, gdown, fastapi, diffusers, xformers, transformers, torchvision, torchaudio, gradio, accelerate, timm, segment-anything-hq, groundingdino-py, controlnet-aux, animatediff\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.97\n",
      "    Uninstalling sentencepiece-0.1.97:\n",
      "      Successfully uninstalled sentencepiece-0.1.97\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 1.12\n",
      "    Uninstalling flatbuffers-1.12:\n",
      "      Successfully uninstalled flatbuffers-1.12\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.1.1\n",
      "    Uninstalling platformdirs-3.1.1:\n",
      "      Successfully uninstalled platformdirs-3.1.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.3.0\n",
      "    Uninstalling fsspec-2023.3.0:\n",
      "      Successfully uninstalled fsspec-2023.3.0\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 18.2.0\n",
      "    Uninstalling attrs-18.2.0:\n",
      "      Successfully uninstalled attrs-18.2.0\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.4.2\n",
      "    Uninstalling typer-0.4.2:\n",
      "      Successfully uninstalled typer-0.4.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.2\n",
      "    Uninstalling requests-2.28.2:\n",
      "      Successfully uninstalled requests-2.28.2\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.9.2\n",
      "    Uninstalling pydantic-1.9.2:\n",
      "      Successfully uninstalled pydantic-1.9.2\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.6.2\n",
      "    Uninstalling anyio-3.6.2:\n",
      "      Successfully uninstalled anyio-3.6.2\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.13.1\n",
      "    Uninstalling huggingface-hub-0.13.1:\n",
      "      Successfully uninstalled huggingface-hub-0.13.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu116\n",
      "    Uninstalling torch-1.12.1+cu116:\n",
      "      Successfully uninstalled torch-1.12.1+cu116\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.12.1\n",
      "    Uninstalling tokenizers-0.12.1:\n",
      "      Successfully uninstalled tokenizers-0.12.1\n",
      "  Attempting uninstall: gdown\n",
      "    Found existing installation: gdown 4.5.1\n",
      "    Uninstalling gdown-4.5.1:\n",
      "      Successfully uninstalled gdown-4.5.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.21.3\n",
      "    Uninstalling transformers-4.21.3:\n",
      "      Successfully uninstalled transformers-4.21.3\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1+cu116\n",
      "    Uninstalling torchvision-0.13.1+cu116:\n",
      "      Successfully uninstalled torchvision-0.13.1+cu116\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.12.1+cu116\n",
      "    Uninstalling torchaudio-0.12.1+cu116:\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu116\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\n",
      "spacy 3.4.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.10.13 which is incompatible.\n",
      "spacy 3.4.1 requires typer<0.5.0,>=0.3.0, but you have typer 0.9.0 which is incompatible.\n",
      "tensorflow 2.9.2 requires flatbuffers<2,>=1.12, but you have flatbuffers 23.5.26 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.25.0 addict-2.4.0 aiofiles-23.2.1 altair-5.2.0 animatediff-0.1.dev267+g7b84f6b.d20231219 antlr4-python3-runtime-4.9.3 anyio-3.7.1 attrs-23.1.0 brotli-1.1.0 cmake-3.28.1 controlnet-aux-0.0.7 decorator-4.4.2 diffusers-0.23.0 einops-0.7.0 exceptiongroup-1.2.0 fastapi-0.105.0 ffmpeg-python-0.2.0 ffmpy-0.3.1 flatbuffers-23.5.26 fsspec-2023.12.2 gdown-4.7.1 gradio-3.50.2 gradio-client-0.6.1 groundingdino-py-0.4.0 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 huggingface-hub-0.17.3 imageio_ffmpeg-0.4.9 importlib-metadata-7.0.0 llvmlite-0.41.1 mediapipe-0.10.9 moviepy-1.0.3 mutagen-1.47.0 ninja-1.11.1.1 numba-0.58.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 onnxruntime-1.16.3 opencv-contrib-python-4.8.1.78 opencv-python-headless-4.8.1.78 orjson-3.9.10 pillow-9.5.0 platformdirs-4.1.0 pooch-1.8.0 proglog-0.1.10 pycocotools-2.0.7 pycryptodomex-3.19.0 pydantic-1.10.13 pydub-0.25.1 pymatting-1.1.12 python-multipart-0.0.6 rembg-2.0.53 requests-2.31.0 safetensors-0.4.1 segment-anything-hq-0.3 semantic-version-2.10.0 sentencepiece-0.1.99 sounddevice-0.4.6 starlette-0.27.0 supervision-0.6.0 timm-0.9.12 tokenizers-0.14.1 tomli-2.0.1 torch-2.1.2 torchaudio-2.1.2 torchvision-0.16.2 transformers-4.34.1 triton-2.1.0 typer-0.9.0 typing-extensions-4.9.0 urllib3-1.26.18 uvicorn-0.24.0.post1 websockets-11.0.3 xformers-0.0.23.post1 yapf-0.40.2 yt-dlp-2023.11.16 zipp-3.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mObtaining file:///storage/aj/animatediff-cli-prompt-travel\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.25.0)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.4.3)\n",
      "Requirement already satisfied: cmake>=3.25.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (3.28.1)\n",
      "Requirement already satisfied: diffusers==0.23.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.23.0)\n",
      "Requirement already satisfied: einops>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.7.0)\n",
      "Requirement already satisfied: gdown>=4.6.6 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (4.7.1)\n",
      "Requirement already satisfied: ninja>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.11.1.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.23.4)\n",
      "Requirement already satisfied: omegaconf>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (2.3.0)\n",
      "Requirement already satisfied: pillow<10.0.0,>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (9.5.0)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.10.13)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (13.3.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.4.1)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.1.99)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.5.0.post1)\n",
      "Requirement already satisfied: torch<2.2.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (2.1.2)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (2.1.2)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.16.2)\n",
      "Requirement already satisfied: transformers<4.35.0,>=4.30.2 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (4.34.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.9.0)\n",
      "Requirement already satisfied: controlnet-aux in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.0.7)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (3.6.1)\n",
      "Requirement already satisfied: ffmpeg-python>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.2.0)\n",
      "Requirement already satisfied: mediapipe in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.10.9)\n",
      "Requirement already satisfied: xformers>=0.0.22.post7 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.0.23.post1)\n",
      "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.26.18)\n",
      "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (2023.11.16)\n",
      "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (3.50.2)\n",
      "Requirement already satisfied: onnxruntime-gpu in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.16.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.5.0)\n",
      "Requirement already satisfied: segment-anything-hq==0.3 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.3)\n",
      "Requirement already satisfied: groundingdino-py==0.4.0 in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (0.4.0)\n",
      "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (3.1.31)\n",
      "Requirement already satisfied: rembg[gpu] in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (2.0.53)\n",
      "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from animatediff==0.1.dev267+g7b84f6b.d20231219) (1.0.3)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (7.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.17.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2022.10.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.31.0)\n",
      "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.4.0)\n",
      "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.40.2)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.9.12)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.6.0.66)\n",
      "Requirement already satisfied: supervision==0.6.0 in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.6.0)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev267+g7b84f6b.d20231219) (23.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.9.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.4.1)\n",
      "Requirement already satisfied: future in /usr/lib/python3/dist-packages (from ffmpeg-python>=0.2.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.18.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.14.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.64.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.11.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf>=2.3.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.0.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.14.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (12.3.101)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<4.35.0,>=4.30.2->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.14.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (8.1.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.9.2)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.19.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.0.10)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.105.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.6.1 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.6.1)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.12.0)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.1.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.9.10)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.24.0.post1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->animatediff==0.1.dev267+g7b84f6b.d20231219) (2022.7.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (23.5.26)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.8.1.78)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.19.6)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.4.6)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.4.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.26.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.4.9)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu->animatediff==0.1.dev267+g7b84f6b.d20231219) (15.0.1)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.17.3)\n",
      "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.16.3)\n",
      "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.8.1.78)\n",
      "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.8.0)\n",
      "Requirement already satisfied: pymatting in /usr/local/lib/python3.10/dist-packages (from rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.1.12)\n",
      "Requirement already satisfied: mutagen in /usr/local/lib/python3.10/dist-packages (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.47.0)\n",
      "Requirement already satisfied: pycryptodomex in /usr/local/lib/python3.10/dist-packages (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.19.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219) (2019.11.28)\n",
      "Requirement already satisfied: brotli in /usr/local/lib/python3.10/dist-packages (from yt-dlp->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.1.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.12.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->animatediff==0.1.dev267+g7b84f6b.d20231219) (5.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->animatediff==0.1.dev267+g7b84f6b.d20231219) (67.6.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.19.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14.0.0,>=13.0.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.8)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.15.1)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.14.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime-gpu->animatediff==0.1.dev267+g7b84f6b.d20231219) (10.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.27.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.23.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (3.17.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (4.1.0)\n",
      "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.58.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=4.6.6->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.7.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (2023.2.28)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->controlnet-aux->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.4.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2.0,>=2.1.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.3.0)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino-py==0.4.0->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.0.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio->animatediff==0.1.dev267+g7b84f6b.d20231219) (1.2.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe->animatediff==0.1.dev267+g7b84f6b.d20231219) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg[gpu]->animatediff==0.1.dev267+g7b84f6b.d20231219) (0.41.1)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: animatediff\n",
      "  Building editable for animatediff (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for animatediff: filename=animatediff-0.1.dev267+g7b84f6b.d20231219-0.editable-py3-none-any.whl size=6608 sha256=5b191f010eca2da8a9eec741afe3f8cb55ea2b15706344d6a429a862879bc121\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0s5iy3ve/wheels/a3/53/5e/a55600e8a54c8af9f77a7cefdd48769ca283a356ff22cc1550\n",
      "Successfully built animatediff\n",
      "Installing collected packages: animatediff\n",
      "  Attempting uninstall: animatediff\n",
      "    Found existing installation: animatediff 0.1.dev267+g7b84f6b.d20231219\n",
      "    Uninstalling animatediff-0.1.dev267+g7b84f6b.d20231219:\n",
      "      Successfully uninstalled animatediff-0.1.dev267+g7b84f6b.d20231219\n",
      "Successfully installed animatediff-0.1.dev267+g7b84f6b.d20231219\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m['/storage/aj/animatediff-cli-prompt-travel', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/storage/aj/animatediff-cli-prompt-travel/src']\n"
     ]
    }
   ],
   "source": [
    "# Optional path settings\n",
    "repo_storage_dir_aj = '/storage/aj'         # Where to store your animatediff-cli-prompt-travel-related files.\n",
    "\n",
    "repo_dir = '/notebooks'    \n",
    "!ln -s /storage/ /notebooks/\n",
    "!ln -s /tmp/ /notebooks\n",
    "\n",
    "%store repo_dir \n",
    "# ===================================================================================================\n",
    "# Save variables to Jupiter's temp storage so we can access it even if the kernel restarts.\n",
    "%store repo_storage_dir_aj\n",
    "\n",
    "#+++GIT更新++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "repo_storage_dir_aj = Path(repo_storage_dir_aj)\n",
    "anime_diff_path = repo_storage_dir_aj / 'animatediff-cli-prompt-travel'\n",
    "\n",
    "%cd \"{Path(repo_storage_dir_aj, 'animatediff-cli-prompt-travel')}\"\n",
    "#!sudo apt-get update\n",
    "#!sudo apt-get install build-essential git yasm cmake libtool libx265-dev -y\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip uninstall onnxruntime onnxruntime-gpu onnxruntime_gpu -y\n",
    "!pip install onnxruntime_gpu\n",
    "#!source venv/bin/activate\n",
    "!pip install -e .\n",
    "!pip install -e .[all]\n",
    "\n",
    "import sys\n",
    "#sys.path.append('/storage/aj/animatediff-cli-prompt-travel/src')\n",
    "print(sys.path)\n",
    "\n",
    "# from animatediff.front import launch\n",
    "# launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7eb6ce-5614-450a-8794-b3207f0e40b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T18:21:59.494480Z",
     "iopub.status.busy": "2023-12-16T18:21:59.493910Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">compositing<span style=\"color: #800080; text-decoration-color: #800080\">  89%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">16/18 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:06</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span> , <span style=\"color: #800000; text-decoration-color: #800000\">2 it/s</span> ]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "compositing\u001b[35m  89%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━\u001b[0m \u001b[32m16/18 \u001b[0m [ \u001b[33m0:00:06\u001b[0m < \u001b[36m0:00:01\u001b[0m , \u001b[31m2 it/s\u001b[0m ]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "composite fg_array is None -> skip\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ffmpeg encoder...\n",
      "Encoding interpolated frames with ffmpeg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from '/storage/aj/animatediff-cli-prompt-travel/stylize/dance00027/cp_2023-12-17_03-03/bg_00_2023-12-17_03-03/%08d.png':\n",
      "  Duration: 00:00:01.20, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 512x904, 15 fps, 15 tbr, 15 tbn, 15 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 (png) -> fps\n",
      "  fps -> Stream #0:0 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x56487e6be100] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "[libx264 @ 0x56487e6be100] profile High, level 3.1\n",
      "[libx264 @ 0x56487e6be100] 264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=6 deblock=1:1:1 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=0.40:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=5 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=15 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=10.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:0.60\n",
      "Output #0, mp4, to '/storage/aj/animatediff-cli-prompt-travel/stylize/dance00027/cp_2023-12-17_03-03/composite2023-12-17_03-14-09.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 512x904, q=-1--1, 15 fps, 15360 tbn, 15 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output to stylize/dance00027/cp_2023-12-17_03-03/composite2023-12-17_03-14-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   18 fps=0.0 q=-1.0 Lsize=     422kB time=00:00:01.00 bitrate=3459.6kbits/s speed=3.89x    \n",
      "video:421kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.233855%\n",
      "[libx264 @ 0x56487e6be100] frame I:2     Avg QP: 6.32  size: 28988\n",
      "[libx264 @ 0x56487e6be100] frame P:5     Avg QP:10.35  size: 30319\n",
      "[libx264 @ 0x56487e6be100] frame B:11    Avg QP:12.16  size: 20109\n",
      "[libx264 @ 0x56487e6be100] consecutive B-frames: 22.2%  0.0% 16.7%  0.0% 27.8% 33.3%\n",
      "[libx264 @ 0x56487e6be100] mb I  I16..4: 45.0% 40.2% 14.9%\n",
      "[libx264 @ 0x56487e6be100] mb P  I16..4:  4.7% 26.1%  8.9%  P16..4: 10.6% 14.2%  8.1%  0.0%  0.0%    skip:27.2%\n",
      "[libx264 @ 0x56487e6be100] mb B  I16..4:  2.5%  5.9%  2.5%  B16..8: 27.7% 23.9%  6.6%  direct: 9.6%  skip:21.3%  L0:49.3% L1:26.9% BI:23.8%\n",
      "[libx264 @ 0x56487e6be100] 8x8 transform intra:53.2% inter:61.5%\n",
      "[libx264 @ 0x56487e6be100] coded y,uvDC,uvAC intra: 57.6% 73.0% 71.5% inter: 42.1% 60.3% 35.1%\n",
      "[libx264 @ 0x56487e6be100] i16 v,h,dc,p: 72%  7%  6% 15%\n",
      "[libx264 @ 0x56487e6be100] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 41% 10% 17%  5%  4%  8%  4%  7%  4%\n",
      "[libx264 @ 0x56487e6be100] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 37% 15% 15%  5%  8%  9%  4%  5%  2%\n",
      "[libx264 @ 0x56487e6be100] i8c dc,h,v,p: 52% 10% 28% 11%\n",
      "[libx264 @ 0x56487e6be100] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x56487e6be100] ref P L0: 57.8%  4.6% 17.8%  7.4%  6.4%  5.9%\n",
      "[libx264 @ 0x56487e6be100] ref B L0: 74.7% 15.5%  6.6%  1.6%  1.5%\n",
      "[libx264 @ 0x56487e6be100] ref B L1: 98.2%  1.8%\n",
      "[libx264 @ 0x56487e6be100] kb/s:2871.83\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/queueing.py\", line 407, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 226, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1550, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1199, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 519, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 512, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 495, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 649, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/storage/aj/animatediff-cli-prompt-travel/src/animatediff/front.py\", line 53, in execute_wrapper\n",
      "    execute_impl(video=saved_file, config=config_path, delete_if_exists=delete_if_exists, is_test=is_test, is_refine=is_refine, bg_config=bg_config)\n",
      "  File \"/storage/aj/animatediff-cli-prompt-travel/src/animatediff/execute.py\", line 138, in execute_impl\n",
      "    create_video_with_audio(final_video_dir, audio, new_file_path)\n",
      "  File \"/storage/aj/animatediff-cli-prompt-travel/src/animatediff/video_utils.py\", line 11, in create_video_with_audio\n",
      "    video_clip = VideoFileClip(input_video)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/video/io/VideoFileClip.py\", line 88, in __init__\n",
      "    self.reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\", line 35, in __init__\n",
      "    infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\", line 244, in ffmpeg_parse_infos\n",
      "    is_GIF = filename.endswith('.gif')\n",
      "AttributeError: 'PosixPath' object has no attribute 'endswith'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fg_フォルダ: stylize/dance00027/fg_00_dance00027/2023-12-17_02-22_00/00-2023-12-17_02-26\n",
      "bg_フォルダ: stylize/dance00027/bg_dance00027/00_img2img\n",
      "final_video_dir: stylize/dance00027/cp_2023-12-17_03-03/composite2023-12-17_03-14-09\n"
     ]
    }
   ],
   "source": [
    "from animatediff.front import launch\n",
    "launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03ad305e-9354-4a72-90b8-801f5732c032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-18T05:25:49.770846Z",
     "iopub.status.busy": "2023-12-18T05:25:49.770334Z",
     "iopub.status.idle": "2023-12-18T05:26:09.561648Z",
     "shell.execute_reply": "2023-12-18T05:26:09.560546Z",
     "shell.execute_reply.started": "2023-12-18T05:25:49.770825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/aj/animatediff-cli-prompt-travel\n",
      "[TikTok] Extracting URL: https://www.tiktok.com/@fukada0318/video/7290017445211819266\n",
      "[TikTok] 7290017445211819266: Downloading video feed\n",
      "[info] 7290017445211819266: Downloading 1 format(s): bytevc1_1080p_2508203-2\n",
      "[download] ./data/video/dance00023.mp4 has already been downloaded\n",
      "[download] 100% of    6.75MiB\n",
      "impl\n",
      "video1: ./data/video/dance00023.mp4\n",
      "config already exists. skip create-config\n",
      "Using generation config: stylize/dance00023/fg_00_dance00023/prompt.json\n",
      "is_sdxl=False\n",
      "is_v2=True\n",
      "Using base model: runwayml/stable-diffusion-v1-5\n",
      "Will save outputs to ./stylize/dance00023/fg_00_dance00023/2023-12-18_13-25-88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing images (animatediff_controlnet): 100%|██████████| 140/140 [00:00<00:00, 962.00it/s]\n",
      "Preprocessing images (controlnet_openpose):   0%|          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing images (controlnet_openpose): 100%|██████████| 140/140 [00:15<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline already loaded, skipping initialization\n",
      "loading c='animatediff_controlnet' model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for conv_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for conv_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for time_embedding.linear_2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.5.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.blocks.5.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_cond_embedding.conv_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.attentions.1.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.downsamplers.0.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.0.downsamplers.0.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.attentions.1.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv_shortcut.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.0.conv_shortcut.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.downsamplers.0.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.1.downsamplers.0.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.attentions.1.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv_shortcut.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.0.conv_shortcut.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.downsamplers.0.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.2.downsamplers.0.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for down_blocks.3.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.5.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.5.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.6.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.6.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.7.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.7.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.8.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.8.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.9.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.9.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.10.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.10.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.11.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_down_blocks.11.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_mid_block.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for controlnet_mid_block.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_in.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_in.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_out.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.attentions.0.proj_out.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.0.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.time_emb_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.time_emb_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2025: UserWarning: for mid_block.resnets.1.conv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
      "\n",
      "loading c='controlnet_openpose' model\n",
      "Pipeline already on the correct device, skipping device transfer\n",
      "c='animatediff_controlnet' / []\n",
      "c='controlnet_openpose' / []\n",
      "Saving prompt config to output directory\n",
      "Initialization complete!\n",
      "Generating 1 animations\n",
      "Running generation 1 of 1\n",
      "Generation seed: 74699447653086840\n",
      "len( region_condi_list )=1\n",
      "len( region_list )=1\n",
      "apply_lcm_lora=True\n",
      "controlnet_for_region=True\n",
      "multi_uncond_mode=True\n",
      "unet_batch_size=1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"LayerNormKernelImpl\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 58\u001b[0m\n\u001b[1;32m     54\u001b[0m anime_diff_path \u001b[38;5;241m=\u001b[39m repo_storage_dir_aj \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manimatediff-cli-prompt-travel\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     56\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mPath(repo_storage_dir_aj, \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124manimatediff-cli-prompt-travel\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m)}\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43murls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_if_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelete_if_exists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_refine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_refine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     62\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/execute.py:49\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(videos, configs, urls, delete_if_exists, is_test, is_refine, bg_config)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/notebooks\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     48\u001b[0m     config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/notebooks\u001b[39m\u001b[38;5;124m\"\u001b[39m):]\n\u001b[0;32m---> 49\u001b[0m \u001b[43mexecute_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaved_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_if_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelete_if_exists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_refine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_refine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbg_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbg_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/execute.py:98\u001b[0m, in \u001b[0;36mexecute_impl\u001b[0;34m(video, config, delete_if_exists, is_test, is_refine, bg_config)\u001b[0m\n\u001b[1;32m     95\u001b[0m save_config_path\u001b[38;5;241m.\u001b[39mwrite_text(model_config\u001b[38;5;241m.\u001b[39mjson(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_test:\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstylize_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstylize_fg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bg_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m         generate(stylize_dir\u001b[38;5;241m=\u001b[39mstylize_bg_dir, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py:615\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(stylize_dir, length, frame_offset)\u001b[0m\n\u001b[1;32m    611\u001b[0m         tmp_config_path\u001b[38;5;241m.\u001b[39mwrite_text(model_config\u001b[38;5;241m.\u001b[39mjson(indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m), encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    612\u001b[0m         config_org \u001b[38;5;241m=\u001b[39m tmp_config_path\n\u001b[0;32m--> 615\u001b[0m     output_0_dir \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_org\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwidth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlength\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverlap\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstylize_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstride\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstylize_dir\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    626\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    628\u001b[0m \u001b[38;5;66;03m#    try:\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m#        # フォルダが存在する場合のみ削除\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m#        shutil.rmtree(output_0_dir.parent / f\"{time_str}_{0:02d}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;66;03m#    except Exception as e:\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m#        print(f\"Error occurred while deleting Output folder: {e}\")\u001b[39;00m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py:445\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(config_path, width, height, length, context, overlap, stride, repeats, device, use_xformers, force_half_vae, out_dir, no_frames, save_merged, version)\u001b[0m\n\u001b[1;32m    440\u001b[0m seed \u001b[38;5;241m=\u001b[39m model_config\u001b[38;5;241m.\u001b[39mseed[idx \u001b[38;5;241m%\u001b[39m num_seeds]\n\u001b[1;32m    442\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration seed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 445\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg2img_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg2img_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_condi_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_condi_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_sdxl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_sdxl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradual_latent_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradual_latent_hires_fix_map\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    479\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py:1541\u001b[0m, in \u001b[0;36mrun_inference\u001b[0;34m(pipeline, n_prompt, seed, steps, guidance_scale, unet_batch_size, width, height, duration, idx, out_dir, context_frames, context_stride, context_overlap, context_schedule, clip_skip, controlnet_map, controlnet_image_map, controlnet_type_map, controlnet_ref_map, controlnet_no_shrink, no_frames, img2img_map, ip_adapter_config_map, region_list, region_condi_list, output_map, is_single_prompt_mode, is_sdxl, apply_lcm_lora, gradual_latent_map)\u001b[0m\n\u001b[1;32m   1538\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;250m \u001b[39mregion_condi_list\u001b[38;5;250m \u001b[39m)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1539\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;250m \u001b[39mregion_list\u001b[38;5;250m \u001b[39m)\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1541\u001b[0m pipeline_output \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43munet_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_stride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_stride\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext_schedule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_type_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_image_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_ref_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_no_shrink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_max_samples_on_vram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_samples_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_samples_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m999\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_max_models_on_vram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_models_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_models_on_vram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontrolnet_is_loop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_loop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_loop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcontrolnet_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg2img_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg2img_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mip_adapter_config_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_condi_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregion_condi_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolation_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapply_lcm_lora\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradual_latent_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradual_latent_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_map\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpreview_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1573\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeneration complete, saving...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1575\u001b[0m save_fn(pipeline_output, out_file\u001b[38;5;241m=\u001b[39mout_file)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py:2481\u001b[0m, in \u001b[0;36mAnimationPipeline.__call__\u001b[0;34m(self, height, width, num_inference_steps, guidance_scale, unet_batch_size, negative_prompt, video_length, num_videos_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, context_frames, context_stride, context_overlap, context_schedule, clip_skip, controlnet_type_map, controlnet_image_map, controlnet_ref_map, controlnet_no_shrink, controlnet_max_samples_on_vram, controlnet_max_models_on_vram, controlnet_is_loop, img2img_map, ip_adapter_config_map, region_list, region_condi_list, interpolation_factor, is_single_prompt_mode, apply_lcm_lora, gradual_latent_map, **kwargs)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;66;03m# 3. Encode input prompt\u001b[39;00m\n\u001b[1;32m   2476\u001b[0m text_encoder_lora_scale \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2477\u001b[0m     cross_attention_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m cross_attention_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2478\u001b[0m )\n\u001b[0;32m-> 2481\u001b[0m prompt_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPromptEncoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#latents_device,\u001b[39;49;00m\n\u001b[1;32m   2485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregion_condi_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_single_prompt_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_uncond_mode\u001b[49m\n\u001b[1;32m   2492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mip_adapter:\n\u001b[1;32m   2495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mip_adapter\u001b[38;5;241m.\u001b[39mdelete_encoder()\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py:101\u001b[0m, in \u001b[0;36mPromptEncoder.__init__\u001b[0;34m(self, pipe, device, latents_device, num_videos_per_prompt, do_classifier_free_guidance, region_condi_list, negative_prompt, is_signle_prompt_mode, clip_skip, multi_uncond_mode)\u001b[0m\n\u001b[1;32m     98\u001b[0m     prompt_nums\u001b[38;5;241m.\u001b[39mappend( \u001b[38;5;28mlen\u001b[39m(_prompt_list) )\n\u001b[1;32m     99\u001b[0m     prompt_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _prompt_list\n\u001b[0;32m--> 101\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_videos_per_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnegative_prompt_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device \u001b[38;5;241m=\u001b[39m latents_device)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_embeds_dtype \u001b[38;5;241m=\u001b[39m prompt_embeds\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance:\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py:821\u001b[0m, in \u001b[0;36mAnimationPipeline._encode_prompt\u001b[0;34m(self, prompt, device, num_videos_per_prompt, do_classifier_free_guidance, negative_prompt, max_embeddings_multiples, prompt_embeds, negative_prompt_embeds, clip_skip)\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_classifier_free_guidance \u001b[38;5;129;01mand\u001b[39;00m negative_prompt_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    819\u001b[0m         negative_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_convert_prompt(negative_prompt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer)\n\u001b[0;32m--> 821\u001b[0m prompt_embeds1, negative_prompt_embeds1 \u001b[38;5;241m=\u001b[39m \u001b[43mget_weighted_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43muncond_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdo_classifier_free_guidance\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_embeddings_multiples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_embeddings_multiples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     prompt_embeds \u001b[38;5;241m=\u001b[39m prompt_embeds1\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/lpw_stable_diffusion.py:343\u001b[0m, in \u001b[0;36mget_weighted_text_embeddings\u001b[0;34m(pipe, prompt, uncond_prompt, max_embeddings_multiples, no_boseos_middle, skip_parsing, skip_weighting, clip_skip)\u001b[0m\n\u001b[1;32m    340\u001b[0m     uncond_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(uncond_tokens, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mpipe\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# get the embeddings\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_unweighted_text_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_max_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_boseos_middle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_boseos_middle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m prompt_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(prompt_weights, dtype\u001b[38;5;241m=\u001b[39mtext_embeddings\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtext_embeddings\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m uncond_prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/lpw_stable_diffusion.py:237\u001b[0m, in \u001b[0;36mget_unweighted_text_embeddings\u001b[0;34m(pipe, text_input, chunk_length, no_boseos_middle, clip_skip)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pipe\u001b[38;5;241m.\u001b[39mtext_encoder, CLIPSkipTextModel):\n\u001b[0;32m--> 237\u001b[0m         text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m         text_embeddings \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mtext_encoder(text_input)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/clip.py:153\u001b[0m, in \u001b[0;36mCLIPSkipTextModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict, clip_skip)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    151\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_skip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_skip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/clip.py:69\u001b[0m, in \u001b[0;36mCLIPSkipTextTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, output_attentions, output_hidden_states, return_dict, clip_skip)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m _expand_mask(attention_mask, hidden_states\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 69\u001b[0m encoder_outputs: BaseModelOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# take the hidden state from the Nth-to-last layer of the encoder, where N = clip_skip\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# clip_skip=1 means take the hidden state from the last layer as with CLIPTextTransformer\u001b[39;00m\n\u001b[1;32m     80\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;241m-\u001b[39mclip_skip]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py:656\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    649\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    650\u001b[0m         create_custom_forward(encoder_layer),\n\u001b[1;32m    651\u001b[0m         hidden_states,\n\u001b[1;32m    652\u001b[0m         attention_mask,\n\u001b[1;32m    653\u001b[0m         causal_attention_mask,\n\u001b[1;32m    654\u001b[0m     )\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 656\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/modeling_clip.py:385\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;124;03m    hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m        returned tensors for more detail.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 385\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    387\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    388\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    389\u001b[0m     causal_attention_mask\u001b[38;5;241m=\u001b[39mcausal_attention_mask,\n\u001b[1;32m    390\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    392\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py:196\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:2543\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2541\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2542\u001b[0m     )\n\u001b[0;32m-> 2543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \"LayerNormKernelImpl\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "############################################################\n",
    "#ここにビデオのURLを入力#######################################\n",
    "urls = [\n",
    "#    \"https://www.tiktok.com/@assen047/video/7273517043876433158\",   #dance00010\n",
    "#    \"https://www.tiktok.com/@xiavigor__/video/7292363704266427653\", #dance00011\n",
    "#    \"https://www.tiktok.com/@admin_break/video/7306911339673324806\", #dance00012\n",
    "#    \"https://www.tiktok.com/@admin_break/video/7309132061330181381\",#dance00013\n",
    "#    \"https://www.tiktok.com/@xntonio9/video/7266524869783784737\",   #dance00014 soccer\n",
    "#    \"https://www.tiktok.com/@chiranjit_babu/video/7309784153761795329\", #dance00015 indo dance ３人でるから没\n",
    " #   \"https://twitter.com/TaichiZhe/status/1732774857807646772\", #dance00016 kick\n",
    " #   \"https://twitter.com/tongbeijapan/status/1731906333081841755\",#dance00017 punch2\n",
    " #   \"https://www.tiktok.com/@chiranjit_babu/video/7287774575062945042\",#dance00018 indo dance2\n",
    " #   \"https://www.tiktok.com/@chiranjit_babu/video/7210939347212700936\", #dance00019 indo dance3    \n",
    "#    \"https://www.tiktok.com/@fantasista283/video/7309087809829162247\", #dance00020 soccer    \n",
    "#    \"https://www.tiktok.com/@lia.lewis/video/7297191511010118945\", #dance00021 soccer girl\n",
    "#    \"https://www.tiktok.com/@raynavallandingham/video/7309946333593439530\", #dance00022 girl kunfoo\n",
    "    \"https://www.tiktok.com/@fukada0318/video/7290017445211819266\", #dance00023 eimi fukada\n",
    "#    \"https://www.tiktok.com/@ai_hinahina/video/7312648648553204999\", #dance00027\n",
    "]\n",
    "videos = [\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004',\n",
    "#    '/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/video/dance31.mp4'\n",
    "#    '/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/video/dance32.mp4',\n",
    "#    '/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/video/dance33.mp4',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00001.mp4',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00002.mp4',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00003.mp4',\n",
    "#     '/notebooks/storage/aj/animatediff-cli-prompt-travel/data/video/dance00004.mp4',\n",
    "]\n",
    "configs = [\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/kobeni.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/real_base.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/just_anime.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/sukuna.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/todo.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/nanamin.json',\n",
    "#    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/jotaro.json',\n",
    "    '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/real_base2.json',\n",
    "]\n",
    "#bg_config = '/notebooks/storage/aj/animatediff-cli-prompt-travel/config/fix/bg/BeachLCM.json'\n",
    "bg_config = None\n",
    "\n",
    "delete_if_exists = False\n",
    "is_test = True\n",
    "is_refine = False\n",
    "############################################################\n",
    "from animatediff.execute import execute\n",
    "from pathlib import Path\n",
    "\n",
    "repo_storage_dir_aj = Path('/storage/aj')\n",
    "anime_diff_path = repo_storage_dir_aj / 'animatediff-cli-prompt-travel'\n",
    "\n",
    "%cd \"{Path(repo_storage_dir_aj, 'animatediff-cli-prompt-travel')}\"\n",
    "\n",
    "execute(videos=videos, configs=configs,urls=urls, delete_if_exists=delete_if_exists, is_test=is_test, is_refine=is_refine, bg_config=bg_config)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"実行時間: {execution_time}秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d338e2-2a39-43d2-9273-e167f5052786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T14:18:06.371418Z",
     "iopub.status.busy": "2023-12-14T14:18:06.371169Z",
     "iopub.status.idle": "2023-12-14T14:33:19.338808Z",
     "shell.execute_reply": "2023-12-14T14:33:19.336588Z",
     "shell.execute_reply.started": "2023-12-14T14:18:06.371394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Preprocessing images (controlnet_openpose)<span style=\"color: #800080; text-decoration-color: #800080\">  11%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">15/140 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:00:34</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:04:24</span> , <span style=\"color: #800000; text-decoration-color: #800000\">0 it/s</span> ]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Preprocessing images (controlnet_openpose)\u001b[35m  11%\u001b[0m \u001b[38;2;249;38;114m━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/140 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:04:24\u001b[0m , \u001b[31m0 it/s\u001b[0m ]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:21:26 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Checking motion module<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                         <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#612\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">612</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:21:26\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Checking motion module\u001b[33m...\u001b[0m                                                         \u001b]8;id=121625;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=169454;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#612\u001b\\\u001b[2m612\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading tokenizer<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                              <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#636\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">636</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading tokenizer\u001b[33m...\u001b[0m                                                              \u001b]8;id=394701;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=752427;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#636\u001b\\\u001b[2m636\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading text encoder<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                           <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#638\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">638</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading text encoder\u001b[33m...\u001b[0m                                                           \u001b]8;id=429254;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=54930;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#638\u001b\\\u001b[2m638\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:21:34 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading VAE<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                    <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#640\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">640</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:21:34\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading VAE\u001b[33m...\u001b[0m                                                                    \u001b]8;id=54579;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=82172;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#640\u001b\\\u001b[2m640\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:21:38 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading UNet<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                                   <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#642\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">642</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:21:38\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading UNet\u001b[33m...\u001b[0m                                                                   \u001b]8;id=223264;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=343764;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#642\u001b\\\u001b[2m642\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:22:12 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">453.</span>20928M-parameter motion module                                             <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">unet.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py#578\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">578</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:22:12\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[1;36m453.\u001b[0m20928M-parameter motion module                                             \u001b]8;id=526978;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py\u001b\\\u001b[2munet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=344862;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py#578\u001b\\\u001b[2m578\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:22:13 </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> gradual_latent_hires_fix enable                                                   <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#656\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">656</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:22:13\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m gradual_latent_hires_fix enable                                                   \u001b]8;id=20994;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=722032;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#656\u001b\\\u001b[2m656\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> model_config.<span style=\"color: #808000; text-decoration-color: #808000\">scheduler</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">DiffusionScheduler.k_dpmpp_2m:</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'k_dpmpp_2m'</span><span style=\"font-weight: bold\">&gt;</span>              <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#657\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">657</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m model_config.\u001b[33mscheduler\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDiffusionScheduler.k_dpmpp_2m:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'k_dpmpp_2m'\u001b[0m\u001b[1m>\u001b[0m              \u001b]8;id=528995;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=516774;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#657\u001b\\\u001b[2m657\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> If you are forced to exit with an error, change to euler_a or lcm                 <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#658\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">658</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m If you are forced to exit with an error, change to euler_a or lcm                 \u001b]8;id=337896;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=93943;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#658\u001b\\\u001b[2m658\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Using scheduler <span style=\"color: #008000; text-decoration-color: #008000\">\"k_dpmpp_2m\"</span> <span style=\"font-weight: bold\">(</span>DPMSolverMultistepScheduler<span style=\"font-weight: bold\">)</span>                        <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#662\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">662</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using scheduler \u001b[32m\"k_dpmpp_2m\"\u001b[0m \u001b[1m(\u001b[0mDPMSolverMultistepScheduler\u001b[1m)\u001b[0m                        \u001b]8;id=288955;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=942412;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#662\u001b\\\u001b[2m662\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading weights from                                                              <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#667\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">667</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span>         <span style=\"color: #800080; text-decoration-color: #800080\">/storage/aj/animatediff-cli-prompt-travel/data/../../../stable-diffusion/stable-d</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span>         <span style=\"color: #800080; text-decoration-color: #800080\">iffusion-webui/models/Stable-diffusion/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">majicmixRealistic_betterV2V25.safetensors</span>  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading weights from                                                              \u001b]8;id=415377;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=639923;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#667\u001b\\\u001b[2m667\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m         \u001b[0m         \u001b[35m/storage/aj/animatediff-cli-prompt-travel/data/../../../stable-diffusion/stable-d\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m         \u001b[0m         \u001b[35miffusion-webui/models/Stable-diffusion/\u001b[0m\u001b[95mmajicmixRealistic_betterV2V25.safetensors\u001b[0m  \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:22:45 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Merging weights into UNet<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                      <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#684\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">684</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:22:45\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Merging weights into UNet\u001b[33m...\u001b[0m                                                      \u001b]8;id=516040;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=32575;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#684\u001b\\\u001b[2m684\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:22:53 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Creating AnimationPipeline<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                     <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#734\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">734</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:22:53\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Creating AnimationPipeline\u001b[33m...\u001b[0m                                                     \u001b]8;id=430686;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=535477;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#734\u001b\\\u001b[2m734\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"color: #808000; text-decoration-color: #808000\">lora_path</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'/storage/aj/animatediff-cli-prompt-travel/data/models/lcm_lora/sd</span> <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">lora.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py#49\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">         </span>         <span style=\"color: #008000; text-decoration-color: #008000\">15/pytorch_lora_weights.safetensors'</span><span style=\"font-weight: bold\">)</span>                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">          </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mlora_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'/storage/aj/animatediff-cli-prompt-travel/data/models/lcm_lora/sd\u001b[0m \u001b]8;id=204837;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=744237;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m         \u001b[0m         \u001b[32m15/pytorch_lora_weights.safetensors'\u001b[0m\u001b[1m)\u001b[0m                                                  \u001b[2m          \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create LoRA network from weights\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc2 (not found in modules_dim)\n",
      "create LoRA for Text Encoder: 0 modules.\n",
      "skipped 72 modules because of missing weight for text encoder.\n",
      "create LoRA for U-Net: 278 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:24:33 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> No TI embeddings found                                                                  <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">ti.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py#104\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">104</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:24:33\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m No TI embeddings found                                                                  \u001b]8;id=655712;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py\u001b\\\u001b[2mti.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=232265;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py#104\u001b\\\u001b[2m104\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:24:35 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> loading <span style=\"color: #808000; text-decoration-color: #808000\">c</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'animatediff_controlnet'</span> model                                          <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">783</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:24:35\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading \u001b[33mc\u001b[0m=\u001b[32m'animatediff_controlnet'\u001b[0m model                                          \u001b]8;id=199815;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=452037;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\u001b\\\u001b[2m783\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">14:24:49 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> loading <span style=\"color: #808000; text-decoration-color: #808000\">c</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'controlnet_openpose'</span> model                                             <a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">generate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">783</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m14:24:49\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading \u001b[33mc\u001b[0m=\u001b[32m'controlnet_openpose'\u001b[0m model                                             \u001b]8;id=124654;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=127945;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\u001b\\\u001b[2m783\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1e8189501d497a9d517bc9570ebd7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ch_model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manimatediff\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfront\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m launch\n\u001b[0;32m----> 3\u001b[0m \u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/storage/aj/animatediff-cli-prompt-travel/src/animatediff/front.py:103\u001b[0m, in \u001b[0;36mlaunch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     iface\u001b[38;5;241m.\u001b[39mlaunch(share\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# def launch():\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m#     # 指定したフォルダ以下のファイル一覧を取得\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#     folder_path = \"/storage/aj/animatediff-cli-prompt-travel/config/fix\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# 無限ループを追加してセルが終了しないようにする\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from animatediff.front import launch\n",
    "\n",
    "launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c5907af-83b2-43ac-92d5-f8ff0c273af7",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-12-13T06:22:15.107738Z",
     "iopub.status.busy": "2023-12-13T06:22:15.106708Z",
     "iopub.status.idle": "2023-12-13T06:22:15.139350Z",
     "shell.execute_reply": "2023-12-13T06:22:15.136887Z",
     "shell.execute_reply.started": "2023-12-13T06:22:15.107648Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">  80%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">12/15 </span> [ <span style=\"color: #808000; text-decoration-color: #808000\">0:18:39</span> &lt; <span style=\"color: #008080; text-decoration-color: #008080\">0:00:11</span> , <span style=\"color: #800000; text-decoration-color: #800000\">0 it/s</span> ]\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m  80%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/15 \u001b[0m [ \u001b[33m0:18:39\u001b[0m < \u001b[36m0:00:11\u001b[0m , \u001b[31m0 it/s\u001b[0m ]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from animatediff.execute import execute\n",
    "import sys\n",
    "import io\n",
    "\n",
    "# Define the function signature\n",
    "def execute_wrapper(configs: list, urls: str, delete_if_exists: bool, is_test: bool, is_refine: bool) -> str:\n",
    "    # Split the inputs by commas and strip whitespace\n",
    "#    videos = [video.strip() for video in videos.split('\\n') if video]\n",
    "#    configs = [config.strip() for config in configs.split('\\n') if config]\n",
    "    config_paths = [os.path.join(\"/storage/aj/animatediff-cli-prompt-travel/config/fix\", config + \".json\") for config in configs]\n",
    "    videos = []\n",
    "    urls = [url.strip() for url in urls.split('\\n') if url]\n",
    "#    bg_config = bg_config.strip()\n",
    "    bg_config = 'NA'\n",
    "    # Redirect stdout to a string stream\n",
    "    old_stdout = sys.stdout\n",
    "    new_stdout = io.StringIO()\n",
    "    sys.stdout = new_stdout\n",
    "    \n",
    "    try:\n",
    "        # Execute the function with the inputs\n",
    "        execute(videos=videos, configs=config_paths, urls=urls, delete_if_exists=delete_if_exists, is_test=is_test, is_refine=is_refine, bg_config=bg_config)\n",
    "    finally:\n",
    "        # Get the console log\n",
    "        console_log = new_stdout.getvalue()\n",
    "        \n",
    "        # Restore stdout\n",
    "        sys.stdout = old_stdout\n",
    "    \n",
    "    return console_log\n",
    "\n",
    "\n",
    "# 指定したフォルダ以下のファイル一覧を取得\n",
    "folder_path = \"/storage/aj/animatediff-cli-prompt-travel/config/fix\"\n",
    "config_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# ファイル名から拡張子を取り除く\n",
    "config_files_without_extension = [os.path.splitext(file)[0] for file in config_files]\n",
    "\n",
    "# ファイル一覧をリストボックスの選択肢に変換\n",
    "file_choices = [(file, file) for file in config_files_without_extension]\n",
    "\n",
    "\n",
    "# Define the Gradio interface\n",
    "iface = gr.Interface(\n",
    "    execute_wrapper, \n",
    "    [\n",
    "#        gr.Textbox(lines=3, placeholder=\"Enter video file paths, separated by commas\", label=\"Videos\"),\n",
    "        gr.CheckboxGroup(file_choices, info=\"please select\", label=\"Configs\"),\n",
    "#        gr.Textbox(lines=3, placeholder=\"Enter config file paths, separated by commas\", label=\"Configs\"),\n",
    "        gr.Textbox(lines=3, placeholder=\"Enter URLs, separated by commas\", label=\"URLs\"),\n",
    "#        gr.Textbox(lines=1, placeholder=\"Enter bg_config file path\", label=\"BG Config\"),\n",
    "        gr.Checkbox(label=\"Delete if exists\"),\n",
    "        gr.Checkbox(label=\"Is test\", value=True),\n",
    "        gr.Checkbox(label=\"Is refine\"),\n",
    "    ],\n",
    "    outputs=\"text\"\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "iface.launch(share=True)\n",
    "\n",
    "# 無限ループを追加してセルが終了しないようにする\n",
    "while True:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adb47e-e655-419a-a3bb-55adfaaa396a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T15:42:38.541311Z",
     "iopub.status.busy": "2023-12-12T15:42:38.540432Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2;36m15:42:48\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mdiffuser_ver\u001b[0m=\u001b[32m'0.23.0'\u001b[0m                               \u001b]8;id=819442;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=491927;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#101\u001b\\\u001b[2m101\u001b[0m\u001b]8;;\u001b\\\n",
      "/usr/local/lib/python3.10/dist-packages/segment_anything_hq/modeling/tiny_vit_sam.py:662: UserWarning: Overwriting tiny_vit_5m_224 in registry with segment_anything_hq.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/segment_anything_hq/modeling/tiny_vit_sam.py:662: UserWarning: Overwriting tiny_vit_11m_224 in registry with segment_anything_hq.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/segment_anything_hq/modeling/tiny_vit_sam.py:662: UserWarning: Overwriting tiny_vit_21m_224 in registry with segment_anything_hq.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/segment_anything_hq/modeling/tiny_vit_sam.py:662: UserWarning: Overwriting tiny_vit_21m_384 in registry with segment_anything_hq.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/segment_anything_hq/modeling/tiny_vit_sam.py:662: UserWarning: Overwriting tiny_vit_21m_512 in registry with segment_anything_hq.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mmask_dir\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' absolute path to mask dir\u001b[0m \u001b]8;id=293385;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py\u001b\\\u001b[2mstylize.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=187538;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py#1427\u001b\\\u001b[2m1427\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[32m(\u001b[0m\u001b[32mthis is optional\u001b[0m\u001b[32m)\u001b[0m\u001b[32m '\u001b[0m\u001b[1m)\u001b[0m not valid -> create mask \u001b[2m               \u001b[0m\n",
      "\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: \n",
      "torch.meshgrid: in an upcoming release, it will be required to pass the indexing\n",
      "argument. (Triggered internally at \n",
      "../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "creating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kfinal text_encoder_type: bert-base-uncased                   \u001b[31mit/s \u001b[0m  \n",
      "creating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K<All keys matched successfully>                              \u001b[31mit/s \u001b[0m  \n",
      "creating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:905:  \n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of \n",
      "Transformers.\n",
      "  warnings.warn(\n",
      "creating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: \u001b[0m  \n",
      "UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or \n",
      "use_reentrant=False explicitly. The default value of use_reentrant will be \n",
      "updated to be False in the future. To maintain current behavior, pass \n",
      "use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to\n",
      "docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "creating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61:  \u001b[0m  \n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "creating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m0/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   0%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m1/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m2/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m3/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   1%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m4/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m5/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   2%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m6/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m7/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m8/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   3%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m9/273\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m10/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m11/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   4%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m12/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m13/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m14/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   5%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m15/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m16/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   6%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m17/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m18/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m19/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   7%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m20/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m21/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m22/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2Kcreating mask from mask_token='person'\u001b[35m   8%\u001b[0m \u001b[90m━━━━━\u001b[0m \u001b[32m22/2…\u001b[0m [ \u001b[33m0:00…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1    \u001b[0m ]\n",
      "                                                                         \u001b[31mit/s \u001b[0m  "
     ]
    }
   ],
   "source": [
    "#!animatediff refine /storage/comfyUI/ComfyUI/output/2023-12-11/ADF/094059 -c /storage/aj/animatediff-cli-prompt-travel/stylize/jojo-dance00017/fg_00_jojo/prompt.json -W 768\n",
    "!animatediff stylize composite /storage/aj/animatediff-cli-prompt-travel/stylize/real_base-dance00022 --simple_composite\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e486d3d-f565-4785-8a72-59c8e126c7bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-11T15:33:53.334762Z",
     "iopub.status.busy": "2023-12-11T15:33:53.334496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2;36m15:34:01\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mdiffuser_ver\u001b[0m=\u001b[32m'0.23.0'\u001b[0m                               \u001b]8;id=101287;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=695536;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#101\u001b\\\u001b[2m101\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using generation config:                           \u001b]8;id=831540;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=357041;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#1085\u001b\\\u001b[2m1085\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         stylize/jojo-dance00020/fg_00_jojo/\u001b[1;36m2023\u001b[0m-\u001b[1;36m12\u001b[0m-11_16-5 \u001b[2m           \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         8_00/prompt.json                                   \u001b[2m           \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Will save outputs to                               \u001b]8;id=805806;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=325025;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#1097\u001b\\\u001b[2m1097\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         .\u001b[35m/refine/\u001b[0m\u001b[95m2023-12-11_23-34-jojo\u001b[0m                     \u001b[2m           \u001b[0m\n",
      "\u001b[2;36m15:34:03\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using generation config:                            \u001b]8;id=581440;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=254093;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#314\u001b\\\u001b[2m314\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         refine/\u001b[1;36m2023\u001b[0m-\u001b[1;36m12\u001b[0m-11_23-\u001b[1;36m34\u001b[0m-jojo/00_prompt.json         \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m15:34:07\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mis_sdxl\u001b[0m=\u001b[3;91mFalse\u001b[0m                                      \u001b]8;id=188667;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py\u001b\\\u001b[2mutil.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=715251;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py#600\u001b\\\u001b[2m600\u001b[0m\u001b]8;;\u001b\\\n"
     ]
    }
   ],
   "source": [
    "#!cp -r /notebooks/storage/comfyUI/ComfyUI/output/2023-12-11/ADF/094059/* /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/jojo-dance00018/face_detailed\n",
    "\n",
    "\n",
    "#!cd /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/jojo-dance00018/face_detailed && for file in _648359646536275_*; do newname=$(echo $file | sed 's/_648359646536275_//; s/_0*/_/'); mv \"$file\" \"$newname\"; done\n",
    "\n",
    "#!cd /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/jojo-dance00018/face_detailed && for file in 000*\\..png; do newname=$(echo $file | sed 's/_/./'); mv \"$file\" \"$newname\"; done\n",
    "\n",
    "#!animatediff stylize composite /storage/aj/animatediff-cli-prompt-travel/stylize/jojo-dance00018 --simple_composite\n",
    "!animatediff refine /storage/aj/animatediff-cli-prompt-travel/stylize/jojo-dance00020/fg_00_jojo/2023-12-11_16-58_00/00-2023-12-11_17-07 -W 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429bc1fa-9b09-4219-9277-7e35d7385446",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-29T16:01:35.548131Z",
     "iopub.status.busy": "2023-11-29T16:01:35.547581Z",
     "iopub.status.idle": "2023-11-29T16:01:36.302703Z",
     "shell.execute_reply": "2023-11-29T16:01:36.301859Z",
     "shell.execute_reply.started": "2023-11-29T16:01:35.548109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"/notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/jjj-dance31/fg_00_jjj/2023-11-29_23-27_dance31.mp4_00/00_2023-11-29_23-30_.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import Video\n",
    "\n",
    "# 透明な動画ファイルのパス\n",
    "transparent_video_path = '/notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/jjj-dance31/fg_00_jjj/2023-11-29_23-27_dance31.mp4_00/00_2023-11-29_23-30_.mp4'\n",
    "\n",
    "# 透明な動画を再生\n",
    "Video(transparent_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e313c7d5-a31f-4959-bf8b-a3ff3b96c990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-14T10:04:37.169062Z",
     "iopub.status.busy": "2023-12-14T10:04:37.168571Z",
     "iopub.status.idle": "2023-12-14T10:06:40.155047Z",
     "shell.execute_reply": "2023-12-14T10:06:40.154443Z",
     "shell.execute_reply.started": "2023-12-14T10:04:37.169038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "/usr/local/lib/python3.10/dist-packages/flax/struct.py:136: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(data_clz, keypaths)\n",
      "\u001b[2;36m10:04:44\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mdiffuser_ver\u001b[0m=\u001b[32m'0.23.0'\u001b[0m                               \u001b]8;id=116641;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=364288;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#101\u001b\\\u001b[2m101\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using generation config:                            \u001b]8;id=847610;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=912925;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#314\u001b\\\u001b[2m314\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         stylize/real_base2-dance00025/fg_00_real_base2/prom \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         pt.json                                             \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mis_sdxl\u001b[0m=\u001b[3;91mFalse\u001b[0m                                      \u001b]8;id=558900;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py\u001b\\\u001b[2mutil.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=325426;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py#600\u001b\\\u001b[2m600\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mis_v2\u001b[0m=\u001b[3;92mTrue\u001b[0m                                         \u001b]8;id=611568;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py\u001b\\\u001b[2mutil.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=196034;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/util.py#578\u001b\\\u001b[2m578\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using base model: runwayml/stable-diffusion-v1-\u001b[1;36m5\u001b[0m    \u001b]8;id=397933;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=257313;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#341\u001b\\\u001b[2m341\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Will save outputs to                                \u001b]8;id=352884;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=899894;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#351\u001b\\\u001b[2m351\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         .\u001b[35m/stylize/real_base2-dance00025/fg_00_real_base2/\u001b[0m\u001b[95m20\u001b[0m \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[95m23-12-14_18-04-88\u001b[0m                                   \u001b[2m          \u001b[0m\n",
      "\u001b[2KPreprocessing images (animatediff_controlnet)\u001b[35m   0%\u001b[0m \u001b[90m━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?  \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (animatediff_controlnet)\u001b[35m   0%\u001b[0m \u001b[90m━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?  \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (animatediff_controlnet)\u001b[35m   5%\u001b[0m \u001b[90m━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?  \u001b[0m ]\n",
      "                                                                           \u001b[31mit…\u001b[0m  \n",
      "\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2K/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_c\n",
      "ollection.py:69: UserWarning: Specified provider 'CUDAExecutionProvider' is not \n",
      "in available provider names.Available providers: 'AzureExecutionProvider, \n",
      "CPUExecutionProvider'\n",
      "  warnings.warn(\n",
      "Preprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m0/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   0%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m1/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m-:-…\u001b[0m , \u001b[31m?   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m2/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m3/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   1%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m4/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m5/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   2%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m6/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m7/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m8/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   3%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m9/2…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m10/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m11/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   4%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m12/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m13/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m14/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   5%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m15/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "\u001b[2K\u001b[1A\u001b[2KPreprocessing images (controlnet_openpose)\u001b[35m   6%\u001b[0m \u001b[90m━━━━\u001b[0m \u001b[32m16/…\u001b[0m [ \u001b[33m0:0…\u001b[0m < \u001b[36m0:0…\u001b[0m , \u001b[31m1   \u001b[0m ]\n",
      "                                                                          \u001b[31mit/s\u001b[0m  \n",
      "\u001b[?25h\u001b[2;36m10:05:03\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Checking motion module\u001b[33m...\u001b[0m                      \u001b]8;id=51432;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=646210;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#612\u001b\\\u001b[2m612\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading tokenizer\u001b[33m...\u001b[0m                           \u001b]8;id=702710;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=486314;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#636\u001b\\\u001b[2m636\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading text encoder\u001b[33m...\u001b[0m                        \u001b]8;id=715073;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=286591;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#638\u001b\\\u001b[2m638\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:05\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading VAE\u001b[33m...\u001b[0m                                 \u001b]8;id=928182;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=766367;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#640\u001b\\\u001b[2m640\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading UNet\u001b[33m...\u001b[0m                                \u001b]8;id=207389;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=251631;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#642\u001b\\\u001b[2m642\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:19\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded \u001b[1;36m453.\u001b[0m20928M-parameter motion module          \u001b]8;id=884207;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py\u001b\\\u001b[2munet.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=230752;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/models/unet.py#578\u001b\\\u001b[2m578\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:20\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m gradual_latent_hires_fix enable                \u001b]8;id=37095;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=478588;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#656\u001b\\\u001b[2m656\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m model_config.\u001b[33mscheduler\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDiffusionScheduler.k_d\u001b[0m \u001b]8;id=333169;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=60529;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#657\u001b\\\u001b[2m657\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[1;95mpmpp_2m:\u001b[0m\u001b[39m \u001b[0m\u001b[32m'k_dpmpp_2m'\u001b[0m\u001b[1m>\u001b[0m                         \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m If you are forced to exit with an error,       \u001b]8;id=403274;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=554568;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#658\u001b\\\u001b[2m658\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         change to euler_a or lcm                       \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using scheduler \u001b[32m\"k_dpmpp_2m\"\u001b[0m                   \u001b]8;id=221630;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=284086;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#662\u001b\\\u001b[2m662\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[1m(\u001b[0mDPMSolverMultistepScheduler\u001b[1m)\u001b[0m                  \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading weights from                           \u001b]8;id=964379;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=519992;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#667\u001b\\\u001b[2m667\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/storage/aj/animatediff-cli-prompt-travel/data\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/../../../stable-diffusion/stable-diffusion-we\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35mbui/models/Stable-diffusion/\u001b[0m\u001b[95mmajicmixRealistic_\u001b[0m \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[95mbetterV2V25.safetensors\u001b[0m                        \u001b[2m               \u001b[0m\n",
      "\u001b[2;36m10:05:23\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Merging weights into UNet\u001b[33m...\u001b[0m                   \u001b]8;id=34568;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=431219;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#684\u001b\\\u001b[2m684\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:25\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Creating AnimationPipeline\u001b[33m...\u001b[0m                  \u001b]8;id=19009;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=689702;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#734\u001b\\\u001b[2m734\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mlora_path\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'/storage/aj/animatediff-cli-pr\u001b[0m \u001b]8;id=321329;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py\u001b\\\u001b[2mlora.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=324894;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/lora.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[32mompt-travel/data/models/lcm_lora/sd15/pytorch_lora_\u001b[0m \u001b[2m          \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[32mweights.safetensors'\u001b[0m\u001b[1m)\u001b[0m                               \u001b[2m          \u001b[0m\n",
      "create LoRA network from weights\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_0_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_1_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_2_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_3_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_4_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_5_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_6_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_7_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_8_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_9_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_10_mlp_fc2 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_k_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_v_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_q_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_self_attn_out_proj (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc1 (not found in modules_dim)\n",
      "skipped lora_te_text_model_encoder_layers_11_mlp_fc2 (not found in modules_dim)\n",
      "create LoRA for Text Encoder: 0 modules.\n",
      "skipped 72 modules because of missing weight for text encoder.\n",
      "create LoRA for U-Net: 278 modules.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "create LoRA network from weights\n",
      "create LoRA for Text Encoder: 72 modules.\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_0_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_1_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_2_downsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_down_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_0_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_1_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_2_upsamplers_0_conv (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_0_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_1_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_up_blocks_3_resnets_2_conv_shortcut (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_0_conv2 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv1 (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_time_emb_proj (not found in modules_dim)\n",
      "skipped lora_unet_mid_block_resnets_1_conv2 (not found in modules_dim)\n",
      "create LoRA for U-Net: 192 modules.\n",
      "skipped 86 modules because of missing weight for U-Net.\n",
      "enable LoRA for text encoder\n",
      "enable LoRA for U-Net\n",
      "\u001b[2;36m10:05:44\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m No TI embeddings found                               \u001b]8;id=404883;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py\u001b\\\u001b[2mti.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=11571;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/ti.py#104\u001b\\\u001b[2m104\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading \u001b[33mc\u001b[0m=\u001b[32m'animatediff_controlnet'\u001b[0m model       \u001b]8;id=519990;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=177548;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\u001b\\\u001b[2m783\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:48\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m loading \u001b[33mc\u001b[0m=\u001b[32m'controlnet_openpose'\u001b[0m model          \u001b]8;id=685411;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=829860;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#783\u001b\\\u001b[2m783\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:49\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Sending pipeline to device \u001b[32m\"cuda\"\u001b[0m               \u001b]8;id=319268;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/pipeline.py\u001b\\\u001b[2mpipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=104318;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/pipeline.py#33\u001b\\\u001b[2m33\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Selected data types: \u001b[33munet_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16,    \u001b]8;id=404810;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py\u001b\\\u001b[2mdevice.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=53984;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[33mtenc_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.float16,                         \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[33mvae_dtype\u001b[0m=\u001b[35mtorch\u001b[0m.bfloat16                          \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Using channels_last memory format for UNet and   \u001b]8;id=143752;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py\u001b\\\u001b[2mdevice.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=934594;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/utils/device.py#111\u001b\\\u001b[2m111\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         VAE                                              \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m10:05:54\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mc\u001b[0m=\u001b[32m'animatediff_controlnet'\u001b[0m \u001b[35m/\u001b[0m \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                     \u001b]8;id=75909;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=196786;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#413\u001b\\\u001b[2m413\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mc\u001b[0m=\u001b[32m'controlnet_openpose'\u001b[0m \u001b[35m/\u001b[0m \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m                        \u001b]8;id=692440;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=565138;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#413\u001b\\\u001b[2m413\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Saving prompt config to output directory            \u001b]8;id=724563;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=667883;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#416\u001b\\\u001b[2m416\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Initialization complete!                            \u001b]8;id=263065;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=386092;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#424\u001b\\\u001b[2m424\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generating \u001b[1;36m1\u001b[0m animations                             \u001b]8;id=956300;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=234721;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#425\u001b\\\u001b[2m425\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Running generation \u001b[1;36m1\u001b[0m of \u001b[1;36m1\u001b[0m                           \u001b]8;id=95747;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=556488;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#435\u001b\\\u001b[2m435\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generation seed: \u001b[1;36m74699447653086840\u001b[0m                  \u001b]8;id=218190;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=781208;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#441\u001b\\\u001b[2m441\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;35mlen\u001b[0m\u001b[1m(\u001b[0m region_condi_list \u001b[1m)\u001b[0m=\u001b[1;36m1\u001b[0m                    \u001b]8;id=53158;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=848234;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1532\u001b\\\u001b[2m1532\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;35mlen\u001b[0m\u001b[1m(\u001b[0m region_list \u001b[1m)\u001b[0m=\u001b[1;36m1\u001b[0m                          \u001b]8;id=147419;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=259069;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1533\u001b\\\u001b[2m1533\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mapply_lcm_lora\u001b[0m=\u001b[3;92mTrue\u001b[0m                          \u001b]8;id=288705;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=344756;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2407\u001b\\\u001b[2m2407\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mcontrolnet_for_region\u001b[0m=\u001b[3;92mTrue\u001b[0m                   \u001b]8;id=486031;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=86655;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2435\u001b\\\u001b[2m2435\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mmulti_uncond_mode\u001b[0m=\u001b[3;92mTrue\u001b[0m                       \u001b]8;id=110847;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=851892;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2436\u001b\\\u001b[2m2436\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33munet_batch_size\u001b[0m=\u001b[1;36m1\u001b[0m                            \u001b]8;id=558369;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=121403;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2437\u001b\\\u001b[2m2437\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:05:55\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1;35mprompt_encoder.get_condi_size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m=\u001b[1;36m2\u001b[0m            \u001b]8;id=895987;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py\u001b\\\u001b[2manimation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=221921;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/pipelines/animation.py#2500\u001b\\\u001b[2m2500\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2K\u001b[35m 100%\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15 \u001b[0m [ \u001b[33m0:00:34\u001b[0m < \u001b[36m0:00:00\u001b[0m , \u001b[31m0 it/s\u001b[0m ] \u001b[31m0 it/s\u001b[0m ]\n",
      "\u001b[?25h\u001b[2;36m10:06:34\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generation complete, saving\u001b[33m...\u001b[0m                \u001b]8;id=467011;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=91221;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1566\u001b\\\u001b[2m1566\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m10:06:36\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Creating ffmpeg encoder\u001b[33m...\u001b[0m                    \u001b]8;id=275420;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=738596;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1442\u001b\\\u001b[2m1442\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Encoding interpolated frames with ffmpeg\u001b[33m...\u001b[0m   \u001b]8;id=496216;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=937916;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1466\u001b\\\u001b[2m1466\u001b[0m\u001b]8;;\u001b\\\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from '/storage/aj/animatediff-cli-prompt-travel/stylize/real_base2-dance00025/fg_00_real_base2/2023-12-14_18-04-88/00-2023-12-14_18-05/%08d.png':\n",
      "  Duration: 00:00:01.07, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 512x896, 15 fps, 15 tbr, 15 tbn, 15 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 (png) -> fps\n",
      "  fps -> Stream #0:0 (libx264)\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mprofile High, level 3.1\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=6 deblock=1:1:1 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=0.40:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=5 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=15 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=10.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:0.60\n",
      "Output #0, mp4, to '/storage/aj/animatediff-cli-prompt-travel/stylize/real_base2-dance00025/fg_00_real_base2/2023-12-14_18-04-88/00_2023-12-14_18-05_.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 512x896, q=-1--1, 15 fps, 15360 tbn, 15 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=   16 fps=0.0 q=-1.0 Lsize=     543kB time=00:00:00.86 bitrate=5131.1kbits/s speed=4.06x    \n",
      "video:542kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.178042%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mframe I:1     Avg QP: 9.83  size: 50224\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mframe P:6     Avg QP: 9.27  size: 38936\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mframe B:9     Avg QP: 9.88  size: 30044\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mconsecutive B-frames: 25.0%  0.0% 18.8% 25.0% 31.2%  0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mmb I  I16..4: 34.1% 41.5% 24.4%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mmb P  I16..4: 28.2% 26.3% 10.0%  P16..4: 13.0% 10.7%  7.0%  0.0%  0.0%    skip: 4.8%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mmb B  I16..4:  5.6%  8.6%  5.1%  B16..8: 24.2% 15.8%  5.7%  direct:23.6%  skip:11.2%  L0:46.3% L1:35.7% BI:18.1%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0m8x8 transform intra:41.9% inter:59.7%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mcoded y,uvDC,uvAC intra: 53.7% 90.4% 88.9% inter: 35.3% 75.7% 58.0%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi16 v,h,dc,p: 83%  8%  3%  5%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 32%  8% 18%  5%  6% 11%  4% 10%  4%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 29% 11% 15%  7%  9% 12%  5%  8%  3%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mi8c dc,h,v,p: 39% 12% 37% 12%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mref P L0: 56.0%  8.3% 20.6%  8.9%  4.9%  1.3%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mref B L0: 67.9% 16.8% 10.4%  4.6%  0.3%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mref B L1: 95.5%  4.5%\n",
      "\u001b[1;36m[libx264 @ 0x55c83b248000] \u001b[0mkb/s:4156.75\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Saved sample to                               \u001b]8;id=168261;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py\u001b\\\u001b[2mgenerate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=252545;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/generate.py#1570\u001b\\\u001b[2m1570\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/storage/aj/animatediff-cli-prompt-travel/sty\u001b[0m \u001b[2m                \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35mlize/real_base2-dance00025/fg_00_real_base2/2\u001b[0m \u001b[2m                \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m023-12-14_18-04-88/\u001b[0m\u001b[95m00_2023-12-14_18-05_\u001b[0m       \u001b[2m                \u001b[0m\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Generation complete!                                \u001b]8;id=770760;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=837148;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#484\u001b\\\u001b[2m484\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Done, exiting\u001b[33m...\u001b[0m                                    \u001b]8;id=927747;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py\u001b\\\u001b[2mcli.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=314663;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/cli.py#490\u001b\\\u001b[2m490\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m        \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Stylized results are output to                  \u001b]8;id=639472;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py\u001b\\\u001b[2mstylize.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=347586;file:///storage/aj/animatediff-cli-prompt-travel/src/animatediff/stylize.py#644\u001b\\\u001b[2m644\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35m/storage/aj/animatediff-cli-prompt-travel/styli\u001b[0m \u001b[2m              \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[35mze/real_base2-dance00025/fg_00_real_base2/\u001b[0m\u001b[95m2023-\u001b[0m \u001b[2m              \u001b[0m\n",
      "\u001b[2;36m         \u001b[0m         \u001b[95m12-14_18-04_00\u001b[0m                                  \u001b[2m              \u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!animatediff stylize create-config /storage/animatediff/animatediff-cli-prompt-travel/data/video/Download.mp4 --fps 25\n",
    "\n",
    "# [3] generate mask\n",
    "#!animatediff stylize create-mask /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-24T02-28-29-mumei-pyrite_v4\n",
    "\n",
    "# The foreground is output to the following directory (FG_STYLYZE_DIR)\n",
    "# STYLYZE_DIR/fg_00_timestamp_str\n",
    "# The background is output to the following directory (BG_STYLYZE_DIR)\n",
    "# STYLYZE_DIR/bg_timestamp_str\n",
    "\n",
    "#!cp -r /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004/00_controlnet_image/controlnet_tile/* /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004/fg_00_Santa/00_controlnet_image/animatediff_controlnet\n",
    "#!cp -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-06T14-46-41-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_tile/* /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-06T14-46-41-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_openpose\n",
    "#!cp -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-05T23-24-25-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_tile/* /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-05T23-24-25-sample-majicmixrealistic_betterv2v25/00_controlnet_image/controlnet_openpose\n",
    "\n",
    "# [4] generate foreground\n",
    "\n",
    "!animatediff stylize generate /storage/aj/animatediff-cli-prompt-travel/stylize/real_base2-dance00025/fg_00_real_base2 -L 16\n",
    "#!rm -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T14-08-07-e-majicmixrealistic_betterv2v25/2023*\n",
    "#!animatediff stylize generate /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T14-08-07-e-majicmixrealistic_betterv2v25 -L 16\n",
    "\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-05T23-24-25-sample-majicmixrealistic_betterv2v25/2023-11-07T17-35-46_00/00-341774366206100 -W 768\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-07T16-42-20-sample-majicmixrealistic_betterv2v25/2023-11-07T18-24-43_00/00-341774366206100 -W 768\n",
    "\n",
    "#!animatediff stylize create-region /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-07T16-42-20-sample-majicmixrealistic_betterv2v25\n",
    "\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T06-42-55-dance22-majicmixrealistic_betterv2v25/2023-11-09T09-41-58_00/00-341774366206100 -W 768\n",
    "#!animatediff refine /storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-09T06-42-55-dance22-majicmixrealistic_betterv2v25/2023-11-09T09-41-58_01/00-341774366206100 -W 1024\n",
    "\n",
    "#!animatediff refine /storage/aj/animatediff-cli-prompt-travel/stylize/jjj-dance33/fg_00_jjj/2023-11-29_01-33_00/00-2023-11-29_01-34 -W 512\n",
    "\n",
    "\n",
    "#!rm -r /notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/bg_work/*\n",
    "#!for file in /notebooks/storage/animatediff/animatediff-cli-prompt-travel/stylize/2023-11-08T15-15-10-d10_msk-majicmixrealistic_betterv2v25/bg_2023-11-08T15-23-08/00_controlnet_image/controlnet_tile/*; do cp \"/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/bg/90000001.png\" \"/notebooks/storage/animatediff/animatediff-cli-prompt-travel/data/bg_work/$(basename \"$file\")\"; done\n",
    "#!animatediff stylize composite /notebooks/storage/aj/animatediff-cli-prompt-travel/stylize/Santa-dance00004"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
